# ExcelMind AI ç¬¬äºŒé˜¶æ®µä¼˜åŒ– - æ¶æ„å¸ˆæ·±åº¦åˆ†ææŠ¥å‘Š

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0
**åˆ†ææ—¥æœŸ**: 2025-01-24
**åˆ†æå¯¹è±¡**: `guanyu2.txt` é«˜çº§é¡¾é—®äº¤æµè®°å½•
**ç³»ç»Ÿæ¶æ„å¸ˆè§†è§’**: æ•´ä½“æ¶æ„è¯„ä¼°ä¸å®æ–½è·¯çº¿å›¾

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

### æ ¸å¿ƒå‘ç°

è¿™ä»½äº¤æµè®°å½•æä¾›äº†**æå…·ä»·å€¼çš„æ¶æ„è®¾è®¡è“å›¾**,å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°† ExcelMind ä»"å•æ­¥ä»£ç ç”Ÿæˆå·¥å…·"å‡çº§ä¸º"æ™ºèƒ½å®¡è®¡åŠ©æ‰‹ç”Ÿæ€ç³»ç»Ÿ"ã€‚é¡¾é—®æå‡ºçš„æ–¹æ¡ˆä¸å½“å‰ç³»ç»Ÿæ¶æ„é«˜åº¦å¥‘åˆ,ä½†éœ€è¦åœ¨ä»¥ä¸‹å…³é”®é¢†åŸŸè¿›è¡Œæ·±åŒ–:

1. âœ… **è™šæ‹Ÿå·¥ä½œå°æ¶æ„** - ä¸ç°æœ‰ `/mnt` è®¾è®¡å®Œç¾åŒ¹é…
2. âœ… **ä¾¦å¯Ÿå…µè„šæœ¬æ¨¡å¼** - å…ƒæ•°æ®æå–ç†å¿µå·²å®æ–½,éœ€æ‰©å±•
3. âœ… **å››é˜¶æ®µæ‰§è¡Œæ¨¡å‹** - éœ€ä¸ç°æœ‰ OTAE å¾ªç¯èåˆ
4. âœ… **Function Calling é€‚é…å™¨** - éœ€æ–°å¢æ™ºèƒ½è°ƒåº¦å±‚
5. âš ï¸ **å†…æ§ä¸‰ç»´æ ¡éªŒ** - å…¨æ–°èƒ½åŠ›,éœ€æ¶æ„çº§æ‰©å±•

### æˆ˜ç•¥ä»·å€¼è¯„ä¼°

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **æ¶æ„å®Œæ•´æ€§** | â­â­â­â­â­ | æä¾›äº†ä»æ•°æ®åˆ°å†³ç­–çš„å®Œæ•´é—­ç¯ |
| **æŠ€æœ¯å¯è¡Œæ€§** | â­â­â­â­â˜† | å¤§éƒ¨åˆ†å¯å®æ–½,éƒ¨åˆ†éœ€æŠ€æœ¯åˆ›æ–° |
| **ä¸šåŠ¡ä»·å€¼** | â­â­â­â­â­ | ç›´æ¥å‘½ä¸­å®¡è®¡å·¥ä½œæ ¸å¿ƒç—›ç‚¹ |
| **å®æ–½å¤æ‚åº¦** | â­â­â­â˜†â˜† | ä¸­ç­‰åé«˜,éœ€åˆ†é˜¶æ®µæ¨è¿› |
| **ä¸ç°æœ‰æ¶æ„å¥‘åˆåº¦** | â­â­â­â­â˜† | é«˜åº¦å…¼å®¹,éœ€å°‘é‡è°ƒæ•´ |

---

## ğŸ—ï¸ ç¬¬ä¸€éƒ¨åˆ†: ç³»ç»Ÿæ¶æ„è¯„ä¼°

### 1.1 è™šæ‹Ÿå·¥ä½œå°è®¾è®¡åˆ†æ

#### é¡¾é—®æå‡ºçš„æ¶æ„

```
æµè§ˆå™¨å†…å­˜ç©ºé—´ (/mnt/)
â”œâ”€â”€ source_A.xlsx (åŸå§‹è´¦å¥—)
â”œâ”€â”€ source_B.xlsx (é“¶è¡Œæµæ°´)
â”œâ”€â”€ template.docx (æŠ¥å‘Šæ¨¡æ¿)
â”œâ”€â”€ rules.docx (åˆ¶åº¦æ–‡æ¡£)
â””â”€â”€ output.xlsx (è®¡ç®—ç»“æœ)
```

#### å½“å‰ç³»ç»Ÿå®ç°çŠ¶æ€

**å·²å®ç°** âœ…:
- `PyodideService` å·²åˆ›å»º `/data`, `/data/temp`, `/output` ç›®å½•
- `FileSystemService` æä¾›æ–‡ä»¶æŒ‚è½½å’Œç®¡ç†
- å…ƒæ•°æ®æå–åŸºç¡€è®¾æ–½å·²å°±ç»ª

**æ¶æ„å·®å¼‚**:
- å½“å‰ä½¿ç”¨ `/data` è€Œé `/mnt` (å‘½åçº¦å®šå·®å¼‚)
- ç¼ºå°‘ç»Ÿä¸€çš„"æ–‡ä»¶è§’è‰²"æ ‡è®°æœºåˆ¶
- ç¼ºå°‘æ–‡ä»¶é—´çš„"å…³ç³»å›¾è°±"

#### æ¶æ„ä¼˜åŒ–å»ºè®®

**æ–¹æ¡ˆA: æœ€å°æ”¹åŠ¨ (æ¨è)**
```typescript
// æ‰©å±•ç°æœ‰ FileSystemService
interface FileRole {
  role: 'source' | 'reference' | 'template' | 'rules' | 'output';
  category?: string; // å¦‚ 'è´¦å¥—', 'æµæ°´', 'æŠ¥å‘Š'
  relationships?: FileRelationship[];
}

interface FileRelationship {
  targetFile: string;
  type: 'validates' | 'populates' | 'references';
  metadata?: Record<string, any>;
}

class EnhancedFileSystemService extends FileSystemService {
  // ç»Ÿä¸€æŒ‚è½½æ¥å£
  async mountWithRole(
    file: File,
    role: FileRole,
    targetPath?: string
  ): Promise<string> {
    const path = targetPath || `/data/${file.name}`;
    // ... æŒ‚è½½é€»è¾‘
    // å»ºç«‹ç´¢å¼•
    this.fileRegistry.set(path, { role, ...metadata });
    return path;
  }

  // ç”Ÿæˆæ–‡ä»¶æ‹“æ‰‘
  getFileTopology(): Graph {
    return this.buildRelationshipGraph(this.fileRegistry);
  }
}
```

**æ–¹æ¡ˆB: å®Œå…¨å¯¹é½ (é•¿æœŸç›®æ ‡)**
- é‡æ„ç›®å½•ç»“æ„ä¸º `/mnt`
- å®ç°æ–‡ä»¶è§’è‰²è‡ªåŠ¨è¯†åˆ«
- æ·»åŠ æ–‡ä»¶ç”Ÿå‘½å‘¨æœŸç®¡ç†

#### å®æ–½ä¼˜å…ˆçº§: **P0 (ç«‹å³å®æ–½)**

ç†ç”±:
1. æ˜¯æ‰€æœ‰åç»­åŠŸèƒ½çš„åŸºç¡€
2. æ”¹åŠ¨é‡å°,é£é™©ä½
3. ç«‹å³æå‡ç”¨æˆ·ä½“éªŒ

---

### 1.2 å¤šæ–‡ä»¶æŒ‚è½½ç­–ç•¥è¯„ä¼°

#### é¡¾é—®çš„æ ¸å¿ƒè®¾è®¡

**å…³é”®æ´å¯Ÿ**:
1. **ä¸Šä¸‹æ–‡"å°è´¦"** - æ˜ç¡®å‘Šè¯‰ AI æ¯ä¸ªæ–‡ä»¶çš„è§’è‰²
2. **Schema æ³¨å…¥** - é¢„æ‰«ææå–è¡¨å¤´å’Œæ ·ä¾‹
3. **è¯­ä¹‰å¯¹é½è¾…åŠ©** - å¸®åŠ© AI ç†è§£åˆ—åæ˜ å°„

#### å½“å‰ç³»ç»Ÿèƒ½åŠ›å¯¹æ¯”

| èƒ½åŠ› | é¡¾é—®æ–¹æ¡ˆ | å½“å‰ç³»ç»Ÿ | å·®è·åˆ†æ |
|------|---------|---------|---------|
| å¤šæ–‡ä»¶ä¸Šä¼  | âœ… æ”¯æŒ | âœ… å·²æ”¯æŒ | æ— å·®è· |
| æ–‡ä»¶è§’è‰²æ ‡è®° | âœ… æ˜¾å¼æ ‡è®° | âŒ éšå¼æ¨æ–­ | **éœ€å¢å¼º** |
| Schema æå– | âœ… ä¾¦å¯Ÿå…µè„šæœ¬ | âš ï¸ éƒ¨åˆ†å®ç° | **éœ€æ‰©å±•** |
| å…ƒæ•°æ®æ³¨å…¥ Prompt | âœ… åŠ¨æ€æ³¨å…¥ | âš ï¸ é™æ€æ¨¡æ¿ | **éœ€ä¼˜åŒ–** |
| å…³ç³»å›¾è°± | âœ… æ–‡ä»¶é—´å…³ç³» | âŒ æ—  | **éœ€æ–°å¢** |

#### æ¶æ„å·®è·åˆ†æ

**1. Schema æ³¨å…¥æœºåˆ¶å·®è·**

**å½“å‰å®ç°** (`excelService.ts`):
```typescript
// ç®€å•çš„é¢„è§ˆç”Ÿæˆ
const preview = await generatePreview(file);
// åªåŒ…å«å‰å‡ è¡Œæ•°æ®,æ— ç±»å‹æ¨æ–­
```

**é¡¾é—®æ–¹æ¡ˆ**:
```python
def extract_excel_metadata(file_paths):
    # 1. è¯»å–æ‰€æœ‰ Sheet åç§°
    # 2. æå–è¡¨å¤´ + å‰3è¡Œæ ·ä¾‹
    # 3. æ•°æ®ç±»å‹æ¨æ–­ (object, int64, float64)
    # 4. æ ¼å¼æ£€æµ‹ (é€—å·ã€ç©ºæ ¼ã€æ—¥æœŸæ ¼å¼)
    # 5. è¿”å›ç»“æ„åŒ– JSON
    return json.dumps(inventory)
```

**å¢å¼ºæ–¹æ¡ˆ**:
```typescript
// æ–°å¢: ExcelMetadataService (services/metadata/)
class ExcelMetadataService {
  async extractDeepMetadata(file: File): Promise<FileMetadata> {
    const result: FileMetadata = {
      filename: file.name,
      sheets: [],
      relationships: [],
      quality: {
        completeness: 0,
        consistency: 0,
        validity: 0
      }
    };

    // ä½¿ç”¨ WASM æ‰§è¡Œæ·±åº¦åˆ†æ
    const metadata = await this.pyodideService.runPython(`
      import pandas as pd
      import json

      def deep_analyze(path):
          xl = pd.ExcelFile(path)
          meta = {
              "sheets": [],
              "global_stats": {
                  "total_sheets": len(xl.sheet_names),
                  "estimated_rows": 0
              }
          }

          for sheet in xl.sheet_names:
              df = pd.read_excel(path, sheet_name=sheet, nrows=100)


              # åˆ—çº§åˆ†æ
              columns = {}
              for col in df.columns:
                  col_meta = {
                      "name": col,
                      "dtype": str(df[col].dtype),
                      "null_ratio": df[col].isna().sum() / len(df),
                      "sample_values": df[col].dropna().head(3).tolist(),
                      "patterns": this.detectPatterns(df[col])
                  }
                  columns[col] = col_meta

              meta["sheets"].append({
                  "name": sheet,
                  "columns": columns,
                  "row_count_sample": len(df)
              })

          return json.dumps(meta)

      deep_analyze("${file.name}")
    `);

    return JSON.parse(metadata);
  }

  private detectPatterns(series: pd.Series): PatternInfo {
    // æ£€æµ‹: æ—¥æœŸæ ¼å¼ã€åƒåˆ†ä½ã€ç™¾åˆ†æ¯”ã€è´§å¸ç¬¦å·ç­‰
  }
}
```

**2. å…ƒæ•°æ®æ³¨å…¥ Prompt å·®è·**

**å½“å‰å®ç°**:
```typescript
// é™æ€æ¨¡æ¿
const prompt = `
è¯·å¤„ç†æ–‡ä»¶: ${file.name}
åŒ…å«åˆ—: ${columns.join(', ')}
`;
```

**é¡¾é—®æ–¹æ¡ˆ**:
```typescript
// åŠ¨æ€æ³¨å…¥
const prompt = `
å½“å‰å·¥ä½œåŒºæ–‡ä»¶ç»“æ„:
${JSON.stringify(metadataJson, null, 2)}

AI æ³¨æ„:
- "é‡‘é¢(å…ƒ)"åˆ—ç±»å‹ä¸º object ä¸”åŒ…å«é€—å·å’Œç©ºæ ¼
- å¤„ç†æ—¶è¯·å…ˆä½¿ç”¨ .str.replace(',', '').str.strip()
- "æ—¥æœŸ"åˆ—å¯èƒ½æ˜¯ Serial Date æ ¼å¼,éœ€ç‰¹æ®Šè½¬æ¢
`;
```

**å®æ–½å»ºè®®**:
```typescript
// æ–°å¢: PromptEnhancementService
class PromptEnhancementService {
  buildEnhancedPrompt(
    basePrompt: string,
    fileMetadata: FileMetadata[]
  ): string {
    const context = this.buildContextBlock(fileMetadata);
    const constraints = this.generateConstraints(fileMetadata);
    const examples = this.generateFewShotExamples(fileMetadata);

    return `
# ç¯å¢ƒä¸Šä¸‹æ–‡
${context}

# æ•°æ®çº¦æŸ
${constraints}

# å‚è€ƒç¤ºä¾‹
${examples}

# ç”¨æˆ·æŒ‡ä»¤
${basePrompt}
    `.trim();
  }

  private buildContextBlock(metadata: FileMetadata[]): string {
    // ç”Ÿæˆæ–‡ä»¶æ¸…å•è¡¨æ ¼
    // æ ‡æ³¨æ¯ä¸ªæ–‡ä»¶çš„è§’è‰²
    // æ˜¾ç¤ºæ–‡ä»¶é—´å…³ç³»
  }

  private generateConstraints(metadata: FileMetadata[]): string {
    // åˆ†ææ•°æ®è´¨é‡é—®é¢˜
    // ç”Ÿæˆé¢„å¤„ç†æŒ‡ä»¤
    // æ ‡æ³¨éœ€è¦æ³¨æ„çš„åˆ—
  }
}
```

#### å®æ–½è·¯çº¿å›¾

**Phase 1: åŸºç¡€å¢å¼º (1-2å‘¨)**
- [ ] æ‰©å±• `FileSystemService` æ”¯æŒæ–‡ä»¶è§’è‰²æ ‡è®°
- [ ] å®ç° `ExcelMetadataService` æ·±åº¦å…ƒæ•°æ®æå–
- [ ] ä¼˜åŒ–å…ƒæ•°æ®æ³¨å…¥åˆ° Prompt çš„é€»è¾‘

**Phase 2: é«˜çº§ç‰¹æ€§ (2-3å‘¨)**
- [ ] å®ç°æ–‡ä»¶å…³ç³»å›¾è°±
- [ ] æ·»åŠ æ•°æ®è´¨é‡è¯„ä¼°
- [ ] å®ç°æ™ºèƒ½çº¦æŸç”Ÿæˆ

**Phase 3: å®Œå–„ä¼˜åŒ– (1å‘¨)**
- [ ] æ€§èƒ½ä¼˜åŒ– (å¤§æ–‡ä»¶å¤„ç†)
- [ ] ç¼“å­˜æœºåˆ¶
- [ ] é”™è¯¯å¤„ç†

---

### 1.3 æ•°æ®æµè½¬æœºåˆ¶åˆ†æ

#### é¡¾é—®çš„ç®¡é“å¼å¤„ç†æµç¨‹

```
åŸå§‹æ•°æ®
  â†“
ã€æ¸…æ´—é˜¶æ®µã€‘
  â†’ cleaned_v.csv
  â†’ cleaned_b.csv
  â†“
ã€å…³è”é˜¶æ®µã€‘
  â†’ merged_result.csv
  â†“
ã€åˆ†æé˜¶æ®µã€‘
  â†’ final_analysis.xlsx
```

**æ ¸å¿ƒä¼˜åŠ¿**:
1. **ä¸­é—´æ€æŒä¹…åŒ–** - æ¯æ­¥ç»“æœå¯å¤ç”¨
2. **æ–­ç‚¹ç»­ä¼ ** - å¤±è´¥åå¯ä»ä¸­é—´æ­¥éª¤æ¢å¤
3. **å¯è¿½æº¯æ€§** - å®Œæ•´çš„æ•°æ®è¡€ç¼˜
4. **å†…å­˜å‹å¥½** - é¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§æ–‡ä»¶

#### å½“å‰ç³»ç»Ÿå®ç°çŠ¶æ€

**ç°æœ‰æµç¨‹**:
```typescript
// å•æ¬¡å¤„ç†
const result = await executeTransformation(code, files);
// ç›´æ¥è¿”å›æœ€ç»ˆç»“æœ,æ— ä¸­é—´æ€
```

**é—®é¢˜**:
- âŒ æ— æ³•æ£€æŸ¥ä¸­é—´æ­¥éª¤
- âŒ å¤±è´¥åéœ€é‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®
- âŒ éš¾ä»¥è°ƒè¯•å¤æ‚é€»è¾‘
- âŒ å†…å­˜å‹åŠ›å¤§

#### æ¶æ„å¢å¼ºæ–¹æ¡ˆ: æ•°æ®æµç¼–æ’å™¨

```typescript
/**
 * æ•°æ®æµç¼–æ’å™¨ - å®ç°ç®¡é“å¼å¤„ç†
 */
class DataFlowOrchestrator {
  private pipeline: PipelineStage[];
  private intermediateResults: Map<string, any>;

  /**
   * å®šä¹‰å¤„ç†ç®¡é“
   */
  definePipeline(stages: PipelineStageDefinition[]): void {
    this.pipeline = stages.map(def => ({
      ...def,
      status: 'pending',
      input: null,
      output: null,
      startTime: null,
      endTime: null,
      error: null
    }));
  }

  /**
   * æ‰§è¡Œç®¡é“
   */
  async execute(
    inputData: any,
    options: {
      resumeFrom?: string;  // æ–­ç‚¹ç»­ä¼ 
      checkpoint?: boolean; // æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹
    } = {}
  ): Promise<PipelineResult> {
    const { resumeFrom, checkpoint = true } = options;

    let currentData = inputData;
    const startIndex = resumeFrom
      ? this.pipeline.findIndex(s => s.id === resumeFrom)
      : 0;

    for (let i = startIndex; i < this.pipeline.length; i++) {
      const stage = this.pipeline[i];

      try {
        // 1. æ‰§è¡Œé˜¶æ®µ
        stage.status = 'running';
        stage.startTime = Date.now();
        stage.input = currentData;

        currentData = await this.executeStage(stage, currentData);

        // 2. ä¿å­˜ä¸­é—´ç»“æœ
        if (checkpoint) {
          await this.saveCheckpoint(stage.id, currentData);
        }

        stage.output = currentData;
        stage.status = 'completed';
        stage.endTime = Date.now();

        this.emit('stageComplete', { stage, result: currentData });

      } catch (error) {
        stage.status = 'failed';
        stage.error = error;
        stage.endTime = Date.now();

        this.emit('stageFailed', { stage, error });

        // æ”¯æŒæ–­ç‚¹ç»­ä¼ 
        throw new PipelineExecutionError(
          `Pipeline failed at stage: ${stage.name}`,
          { failedStage: stage.id, canResume: true }
        );
      }
    }

    return {
      success: true,
      finalOutput: currentData,
      stages: this.pipeline,
      executionTime: this.calculateTotalTime()
    };
  }

  /**
   * æ‰§è¡Œå•ä¸ªé˜¶æ®µ
   */
  private async executeStage(
    stage: PipelineStage,
    inputData: any
  ): Promise<any> {
    // æ ¹æ® stage ç±»å‹é€‰æ‹©æ‰§è¡Œå™¨
    switch (stage.type) {
      case 'cleaning':
        return await this.executeCleaningStage(stage, inputData);
      case 'transformation':
        return await this.executeTransformationStage(stage, inputData);
      case 'analysis':
        return await this.executeAnalysisStage(stage, inputData);
      case 'validation':
        return await this.executeValidationStage(stage, inputData);
      default:
        throw new Error(`Unknown stage type: ${stage.type}`);
    }
  }

  /**
   * ä¿å­˜æ£€æŸ¥ç‚¹
   */
  private async saveCheckpoint(
    stageId: string,
    data: any
  ): Promise<void> {
    // ä¿å­˜åˆ°è™šæ‹Ÿæ–‡ä»¶ç³»ç»Ÿ
    const checkpointPath = `/data/temp/checkpoint_${stageId}.json`;
    await this.fileSystem.writeFile(
      checkpointPath,
      JSON.stringify(data)
    );

    // è®°å½•åˆ°å…ƒæ•°æ®
    this.checkpoints.set(stageId, {
      path: checkpointPath,
      timestamp: Date.now(),
      size: JSON.stringify(data).length
    });
  }

  /**
   * åŠ è½½æ£€æŸ¥ç‚¹
   */
  async loadCheckpoint(stageId: string): Promise<any> {
    const checkpoint = this.checkpoints.get(stageId);
    if (!checkpoint) {
      throw new Error(`No checkpoint found for stage: ${stageId}`);
    }

    const data = await this.fileSystem.readFile(checkpoint.path);
    return JSON.parse(data);
  }
}

/**
 * ç®¡é“é˜¶æ®µå®šä¹‰
 */
interface PipelineStageDefinition {
  id: string;
  name: string;
  type: 'cleaning' | 'transformation' | 'analysis' | 'validation';
  executor: string; // Python ä»£ç æˆ–å‡½æ•°å¼•ç”¨
  config?: any;
}

/**
 * ç®¡é“é˜¶æ®µå®ä¾‹
 */
interface PipelineStage extends PipelineStageDefinition {
  status: 'pending' | 'running' | 'completed' | 'failed';
  input: any;
  output: any;
  startTime: number | null;
  endTime: number | null;
  error: Error | null;
}
```

#### ä½¿ç”¨ç¤ºä¾‹

```typescript
// å®šä¹‰å¤šè¡¨å¯¹è´¦ç®¡é“
const orchestrator = new DataFlowOrchestrator(pyodideService);

orchestrator.definePipeline([
  {
    id: 'clean_voucher',
    name: 'æ¸…æ´—å‡­è¯è¡¨',
    type: 'cleaning',
    executor: `
df = pd.read_excel('/mnt/voucher.xlsx')
df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
df['å€Ÿæ–¹é‡‘é¢'] = df['å€Ÿæ–¹é‡‘é¢'].astype(str).str.replace(',', '').astype(float)
df_clean = df.dropna(subset=['å‡­è¯å·'])
df_clean.to_csv('/data/temp/cleaned_voucher.csv', index=False)
df_clean
    `
  },
  {
    id: 'clean_bank',
    name: 'æ¸…æ´—é“¶è¡Œæµæ°´',
    type: 'cleaning',
    executor: `
df = pd.read_excel('/mnt/bank.xlsx')
df['äº¤æ˜“æ—¥æœŸ'] = pd.to_datetime(df['äº¤æ˜“æ—¥æœŸ'])
df['æ”¯å‡ºé‡‘é¢'] = df['æ”¯å‡ºé‡‘é¢'].astype(str).str.replace(',', '').str.replace('-', '').astype(float)
df_clean = df.dropna(subset=['æµæ°´å·'])
df_clean.to_csv('/data/temp/cleaned_bank.csv', index=False)
df_clean
    `
  },
  {
    id: 'merge_data',
    name: 'å…³è”åŒ¹é…',
    type: 'transformation',
    executor: `
voucher = pd.read_csv('/data/temp/cleaned_voucher.csv')
bank = pd.read_csv('/data/temp/cleaned_bank.csv')

# æ¨¡ç³ŠåŒ¹é…
merged = pd.merge(
    voucher,
    bank,
    left_on=['å€Ÿæ–¹é‡‘é¢', 'æ—¥æœŸ'],
    right_on=['æ”¯å‡ºé‡‘é¢', 'äº¤æ˜“æ—¥æœŸ'],
    how='outer',
    indicator=True
)
merged.to_excel('/data/temp/merged_result.xlsx', index=False)
merged
    `
  },
  {
    id: 'analyze_discrepancies',
    name: 'åˆ†æå·®å¼‚',
    type: 'analysis',
    executor: `
df = pd.read_excel('/data/temp/merged_result.xlsx')

discrepancies = df[df['_merge'] != 'both']
summary = {
    'total_voucher': len(df),
    'total_bank': len(df[df['_merge'] == 'right_only']),
    'matched': len(df[df['_merge'] == 'both']),
    'unmatched_voucher': len(df[df['_merge'] == 'left_only']),
    'unmatched_bank': len(df[df['_merge'] == 'right_only']),
    'discrepancy_list': discrepancies.to_dict('records')
}

import json
with open('/data/temp/analysis_result.json', 'w') as f:
    json.dump(summary, f)

summary
    `
  }
]);

// æ‰§è¡Œç®¡é“
try {
  const result = await orchestrator.execute(initialData, {
    checkpoint: true  // ä¿å­˜æ£€æŸ¥ç‚¹
  });

  console.log('Pipeline completed:', result.finalOutput);

} catch (error) {
  if (error.canResume) {
    // ä»å¤±è´¥ç‚¹æ¢å¤
    const resumeResult = await orchestrator.execute(null, {
      resumeFrom: error.failedStage,
      checkpoint: true
    });
  }
}
```

#### å®æ–½ä¼˜å…ˆçº§: **P0 (æ ¸å¿ƒåŠŸèƒ½)**

ç†ç”±:
1. å¤§å¹…æå‡ç³»ç»Ÿå¯é æ€§
2. æ”¹å–„è°ƒè¯•ä½“éªŒ
3. æ”¯æŒå¤æ‚å®¡è®¡æµç¨‹
4. ä¸ç°æœ‰æ¶æ„å®Œç¾å¥‘åˆ

---

### 1.4 æ€»æ§å¼•æ“è®¾è®¡è¯„ä¼°

#### é¡¾é—®çš„å››é˜¶æ®µæ‰§è¡Œæ¨¡å‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¬¬ä¸€é˜¶æ®µ: ç¯å¢ƒä¾¦å¯Ÿ (Scouting)           â”‚
â”‚  â†’ Excel å…ƒæ•°æ®æå–                     â”‚
â”‚  â†’ Word ç»“æ„åˆ†æ                        â”‚
â”‚  â†’ è§„åˆ™æ–‡æ¡£è§£æ                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç¬¬äºŒé˜¶æ®µ: å†…æ§é¢„å®¡ (Pre-Filtering)      â”‚
â”‚  â†’ è§„åˆ™æå–                             â”‚
â”‚  â†’ å¼‚å¸¸æ•°æ®ç­›é€‰                         â”‚
â”‚  â†’ é£é™©è¯„åˆ†                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç¬¬ä¸‰é˜¶æ®µ: AI æ·±åº¦å®¡è®¡ (AI Reasoning)    â”‚
â”‚  â†’ å¤šç»´äº¤å‰éªŒè¯                         â”‚
â”‚  â†’ çŸ›ç›¾è¯†åˆ«                             â”‚
â”‚  â†’ å»ºè®®ç”Ÿæˆ                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç¬¬å››é˜¶æ®µ: æˆæœè¾“å‡º (Generating)         â”‚
â”‚  â†’ Word è‡ªåŠ¨å¡«å……                        â”‚
â”‚  â†’ æŠ¥å‘Šç”Ÿæˆ                             â”‚
â”‚  â†’ ä¸‹è½½é“¾æ¥                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä¸ç°æœ‰ OTAE å¾ªç¯çš„æ˜ å°„

| é¡¾é—®é˜¶æ®µ | OTAE é˜¶æ®µ | å½“å‰å®ç°çŠ¶æ€ | èåˆæ–¹æ¡ˆ |
|---------|----------|-------------|---------|
| ç¯å¢ƒä¾¦å¯Ÿ | Observe | âœ… éƒ¨åˆ†å®ç° | **éœ€æ‰©å±•** |
| å†…æ§é¢„å®¡ | Think | âš ï¸ ç¼ºå¤± | **éœ€æ–°å¢** |
| AI æ·±åº¦å®¡è®¡ | Think | âš ï¸ åŸºç¡€å®ç° | **éœ€å¢å¼º** |
| æˆæœè¾“å‡º | Act | âœ… å·²å®ç° | **ä¿æŒ** |
| - | Evaluate | âœ… å·²å®ç° | **ä¿æŒ** |

#### æ¶æ„èåˆæ–¹æ¡ˆ: å¢å¼ºå‹ AgenticOrchestrator

```typescript
/**
 * å¢å¼ºå‹å¤šæ­¥åˆ†æç¼–æ’å™¨
 * èåˆé¡¾é—®çš„å››é˜¶æ®µæ¨¡å‹ä¸ OTAE å¾ªç¯
 */
class EnhancedAgenticOrchestrator extends AgenticOrchestrator {

  /**
   * æ‰§è¡Œå®Œæ•´çš„å®¡è®¡å·¥ä½œæµ
   */
  public async executeAuditWorkflow(
    userPrompt: string,
    files: DataFileInfo[],
    options?: {
      enableInternalControl?: boolean; // æ˜¯å¦å¯ç”¨å†…æ§æ¨¡å¼
      auditDepth?: 'basic' | 'standard' | 'deep';
    }
  ): Promise<TaskResult> {
    const workflowId = this.generateId();

    this.log('info', 'Starting audit workflow', {
      workflowId,
      prompt: userPrompt,
      fileCount: files.length,
      mode: options?.enableInternalControl ? 'å†…æ§è¯„ä»·æ¨¡å¼' : 'æ ‡å‡†æ¨¡å¼'
    });

    try {
      // ========== Phase 1: ç¯å¢ƒä¾¦å¯Ÿ ==========
      this.notifyProgress('æ­£åœ¨åˆ†ææ–‡æ¡£ç»“æ„...');

      const scoutingResult = await this.executeScoutingPhase(files);

      if (!scoutingResult.success) {
        throw new Error('Scouting phase failed');
      }

      // ========== Phase 2: å†…æ§é¢„å®¡ (å¯é€‰) ==========
      let preFilterResult: PreFilterResult | null = null;

      if (options?.enableInternalControl) {
        this.notifyProgress('æ­£åœ¨æ‰§è¡Œå†…æ§é¢„å®¡...');

        preFilterResult = await this.executePreFilterPhase(
          scoutingResult.metadata
        );

        if (!preFilterResult.success) {
          this.warn('Pre-filter failed, continuing without it');
        }
      }

      // ========== Phase 3: AI æ·±åº¦å®¡è®¡ ==========
      this.notifyProgress('AI æ­£åœ¨è¿›è¡Œæ·±åº¦åˆ†æ...');

      const analysisResult = await this.executeAIAnalysisPhase({
        userPrompt,
        scoutingData: scoutingResult.metadata,
        preFilterData: preFilterResult?.exceptions,
        auditDepth: options?.auditDepth || 'standard'
      });

      if (!analysisResult.success) {
        throw new Error('AI analysis phase failed');
      }

      // ========== Phase 4: æˆæœè¾“å‡º ==========
      this.notifyProgress('æ­£åœ¨ç”Ÿæˆå®¡è®¡æŠ¥å‘Š...');

      const generationResult = await this.executeGenerationPhase({
        analysis: analysisResult,
        scouting: scoutingResult,
        preFilter: preFilterResult
      });

      if (!generationResult.success) {
        throw new Error('Generation phase failed');
      }

      // ========== æ±‡æ€»ç»“æœ ==========
      return this.buildFinalResult({
        scouting: scoutingResult,
        preFilter: preFilterResult,
        analysis: analysisResult,
        generation: generationResult
      });

    } catch (error) {
      this.log('error', 'Audit workflow failed', { error });
      return this.handleTaskFailure(error as Error);
    }
  }

  /**
   * Phase 1: ç¯å¢ƒä¾¦å¯Ÿ
   * æå–æ‰€æœ‰æ–‡ä»¶çš„å…ƒæ•°æ®å’Œç»“æ„ä¿¡æ¯
   */
  private async executeScoutingPhase(
    files: DataFileInfo[]
  ): Promise<ScoutingResult> {
    const startTime = Date.now();

    try {
      // 1. æŒ‚è½½æ–‡ä»¶åˆ°è™šæ‹Ÿæ–‡ä»¶ç³»ç»Ÿ
      const mountedFiles = await this.mountFiles(files);

      // 2. å¹¶è¡Œæ‰§è¡Œä¾¦å¯Ÿ
      const [excelMetadata, wordMetadata, rulesMetadata] = await Promise.all([
        this.scoutExcelFiles(mountedFiles.filter(f => f.type === 'excel')),
        this.scoutWordFiles(mountedFiles.filter(f => f.type === 'word')),
        this.extractRulesFromDocuments(mountedFiles.filter(f => f.category === 'rules'))
      ]);

      // 3. æ„å»ºæ–‡ä»¶å…³ç³»å›¾è°±
      const relationshipGraph = this.buildRelationshipGraph([
        ...excelMetadata,
        ...wordMetadata,
        ...rulesMetadata
      ]);

      return {
        success: true,
        metadata: {
          excel: excelMetadata,
          word: wordMetadata,
          rules: rulesMetadata,
          relationships: relationshipGraph
        },
        executionTime: Date.now() - startTime
      };

    } catch (error) {
      return {
        success: false,
        error: error as Error,
        executionTime: Date.now() - startTime
      };
    }
  }

  /**
   * Phase 2: å†…æ§é¢„å®¡
   * æ ¹æ®è§„åˆ™ç­›é€‰å¼‚å¸¸æ•°æ®
   */
  private async executePreFilterPhase(
    scoutingData: ScoutingMetadata
  ): Promise<PreFilterResult> {
    const startTime = Date.now();

    try {
      // 1. æå–å†…æ§è§„åˆ™
      const rules = await this.extractInternalControlRules(
        scoutingData.rules
      );

      // 2. æ‰§è¡Œå¼‚å¸¸ç­›é€‰ (ä½¿ç”¨ WASM)
      const exceptions = await this.runPreFilterEngine({
        rules: rules,
        excelFiles: scoutingData.excel
      });

      // 3. é£é™©è¯„åˆ†
      const riskScores = await this.calculateRiskScores(exceptions);

      return {
        success: true,
        exceptions: exceptions,
        riskScores: riskScores,
        rulesCount: rules.length,
        executionTime: Date.now() - startTime
      };

    } catch (error) {
      return {
        success: false,
        error: error as Error,
        executionTime: Date.now() - startTime
      };
    }
  }

  /**
   * Phase 3: AI æ·±åº¦å®¡è®¡
   * å¤šç»´äº¤å‰éªŒè¯ä¸åˆ†æ
   */
  private async executeAIAnalysisPhase(
    context: AnalysisContext
  ): Promise<AnalysisResult> {
    const startTime = Date.now();

    try {
      // 1. æ„å»ºå¢å¼º Prompt
      const enhancedPrompt = this.buildAnalysisPrompt(context);

      // 2. è°ƒç”¨ AI åˆ†æ (æ”¯æŒå¤šè½®)
      const aiResponse = await this.aiService.analyze({
        prompt: enhancedPrompt,
        context: context,
        maxRounds: 3  // å…è®¸è‡ªæˆ‘ä¿®æ­£
      });

      // 3. éªŒè¯ AI è¾“å‡º
      const validationResult = await this.validateAIOutput(
        aiResponse,
        context
      );

      if (!validationResult.isValid) {
        // è§¦å‘è‡ªæˆ‘ä¿®å¤
        const correctedResponse = await this.correctAIOutput(
          aiResponse,
          validationResult.errors
        );
        return {
          success: true,
          analysis: correctedResponse,
          validation: validationResult,
          executionTime: Date.now() - startTime
        };
      }

      return {
        success: true,
        analysis: aiResponse,
        validation: validationResult,
        executionTime: Date.now() - startTime
      };

    } catch (error) {
      return {
        success: false,
        error: error as Error,
        executionTime: Date.now() - startTime
      };
    }
  }

  /**
   * Phase 4: æˆæœè¾“å‡º
   * ç”Ÿæˆæœ€ç»ˆæ–‡æ¡£å’ŒæŠ¥å‘Š
   */
  private async executeGenerationPhase(
    context: GenerationContext
  ): Promise<GenerationResult> {
    const startTime = Date.now();

    try {
      // 1. æ•°æ®å‡†å¤‡
      const preparedData = await this.prepareDataForGeneration(context);

      // 2. æ–‡æ¡£å¡«å……
      const filledDocument = await this.fillWordTemplate({
        template: context.scouting.word[0],  // å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯æ¨¡æ¿
        data: preparedData
      });

      // 3. ç”Ÿæˆå®¡è®¡æŠ¥å‘Š (å¯é€‰)
      const auditReport = await this.generateAuditReport(context);

      // 4. åˆ›å»ºä¸‹è½½é“¾æ¥
      const downloadUrl = await this.createDownloadLink(filledDocument);

      return {
        success: true,
        document: filledDocument,
        report: auditReport,
        downloadUrl: downloadUrl,
        executionTime: Date.now() - startTime
      };

    } catch (error) {
      return {
        success: false,
        error: error as Error,
        executionTime: Date.now() - startTime
      };
    }
  }

  /**
   * æ„å»ºåˆ†æ Prompt
   */
  private buildAnalysisPrompt(context: AnalysisContext): string {
    const sections = [];

    // 1. è§’è‰²å®šä¹‰
    sections.push(`
# è§’è‰²
ä½ æ˜¯ä¸€ä½å…·å¤‡ CIA å’Œ CISA èµ„è´¨çš„èµ„æ·±å®¡è®¡ç»ç†ã€‚
ä½ æ“…é•¿ä»å¤šç»´åº¦æ•°æ®è¿›è¡Œäº¤å‰éªŒè¯,å‘ç°åˆè§„æ€§é£é™©ã€‚
    `);

    // 2. ä»»åŠ¡ç›®æ ‡
    sections.push(`
# ä»»åŠ¡
${context.userPrompt}

å®¡è®¡æ·±åº¦: ${context.auditDepth}
    `);

    // 3. æ•°æ®ä¸Šä¸‹æ–‡
    if (context.scoutingData) {
      sections.push(`
# æ•°æ®ä¸Šä¸‹æ–‡
${this.formatScoutingData(context.scoutingData)}
      `);
    }

    // 4. å†…æ§è§„åˆ™ (å¦‚æœæœ‰)
    if (context.preFilterData) {
      sections.push(`
# å†…æ§è§„åˆ™
${this.formatRules(context.preFilterData)}

# é¢„å®¡å‘ç°çš„å¼‚å¸¸
${this.formatExceptions(context.preFilterData)}
      `);
    }

    // 5. åˆ†æè¦æ±‚
    sections.push(`
# åˆ†æè¦æ±‚
è¯·æŒ‰ä»¥ä¸‹ç»´åº¦è¿›è¡Œç»¼åˆå®¡è®¡:

1. äº‹å®æ¯”å¯¹: æ•°æ®æ˜¯å¦è¿åè§„åˆ™?
2. é™ˆè¿°å¤æ ¸: æŠ¥å‘Šæè¿°æ˜¯å¦å®¢è§‚?
3. é£é™©é‡åŒ–: è®¡ç®—é£é™©è¯„åˆ†

è¾“å‡ºæ ¼å¼: JSON
{
  "findings": [...],
  "risk_assessment": {...},
  "recommendations": [...],
  "conflicts": [...]
}
    `);

    return sections.join('\n\n');
  }
}
```

#### å…³é”®æ”¹è¿›ç‚¹

1. **æ˜ç¡®çš„é˜¶æ®µåˆ’åˆ†** - æ˜“äºç†è§£å’Œè°ƒè¯•
2. **å¯é€‰çš„å†…æ§æ¨¡å¼** - çµæ´»æ”¯æŒä¸åŒåœºæ™¯
3. **å¹¶è¡Œä¾¦å¯Ÿ** - æå‡æ€§èƒ½
4. **å…³ç³»å›¾è°±** - æ”¯æŒå¤æ‚æ–‡æ¡£å…³è”
5. **å¤šè½® AI äº¤äº’** - æå‡åˆ†æè´¨é‡
6. **å®Œæ•´çš„é”™è¯¯å¤„ç†** - æ¯ä¸ªé˜¶æ®µç‹¬ç«‹å®¹é”™

#### å®æ–½ä¼˜å…ˆçº§: **P0 (æ ¸å¿ƒæ¼”è¿›)**

ç†ç”±:
1. æ˜¯ç³»ç»Ÿä»"å·¥å…·"åˆ°"åŠ©æ‰‹"çš„å…³é”®å‡çº§
2. æ¶æ„å…¼å®¹æ€§å¥½,å¯æ¸è¿›å¼å®æ–½
3. ä¸šåŠ¡ä»·å€¼æ˜¾è‘—æå‡

---

## ğŸ”§ ç¬¬äºŒéƒ¨åˆ†: æ ¸å¿ƒæ¨¡å—è¯†åˆ«

### 2.1 ä¾¦å¯Ÿå…µè„šæœ¬ (Scout Scripts)

#### Excel ä¾¦å¯Ÿå…µ

**é¡¾é—®æä¾›çš„ Python å®ç°**:
```python
import pandas as pd
import json
import os

def extract_excel_metadata(file_paths):
    inventory = []
    for path in file_paths:
        file_name = os.path.basename(path)
        try:
            xl = pd.ExcelFile(path)
            sheets = xl.sheet_names

            sheet_info = []
            for sheet in sheets:
                df = pd.read_excel(path, sheet_name=sheet, nrows=3)

                col_details = {}
                for col in df.columns:
                    sample_val = str(df[col].dropna().iloc[0]) if not df[col].dropna().empty else "None"
                    col_details[col] = {
                        "dtype": str(df[col].dtype),
                        "sample": sample_val
                    }

                sheet_info.append({
                    "sheet_name": sheet,
                    "columns": list(df.columns),
                    "column_details": col_details
                })

            inventory.append({
                "filename": file_name,
                "full_path": path,
                "sheets": sheet_info
            })
        except Exception as e:
            inventory.append({"filename": file_name, "error": str(e)})

    return json.dumps(inventory, ensure_ascii=False)
```

#### å½“å‰ç³»ç»Ÿå¯¹æ¯”

| åŠŸèƒ½ | é¡¾é—®æ–¹æ¡ˆ | å½“å‰ç³»ç»Ÿ | å·®è· |
|------|---------|---------|------|
| åŸºç¡€å…ƒæ•°æ®æå– | âœ… | âœ… | æ—  |
| å¤š Sheet æ”¯æŒ | âœ… | âš ï¸ éƒ¨åˆ† | **éœ€å®Œå–„** |
| æ•°æ®ç±»å‹æ¨æ–­ | âœ… | âŒ | **éœ€æ–°å¢** |
| æ ·ä¾‹æ•°æ®æå– | âœ… (3è¡Œ) | âš ï¸ (å˜é•¿) | éœ€æ ‡å‡†åŒ– |
| æ ¼å¼æ¨¡å¼æ£€æµ‹ | âŒ | âŒ | **å…±åŒç¼ºå¤±** |
| é”™è¯¯å¤„ç† | âœ… | âš ï¸ | éœ€åŠ å¼º |
| æ€§èƒ½ä¼˜åŒ– | âœ… (nrows=3) | âš ï¸ | éœ€ä¼˜åŒ– |

#### å¢å¼ºå®ç°æ–¹æ¡ˆ

```typescript
/**
 * Excel æ·±åº¦ä¾¦å¯ŸæœåŠ¡
 */
class ExcelScoutService {
  constructor(
    private pyodideService: PyodideService,
    private cacheService: CacheService
  ) {}

  /**
   * æ·±åº¦æ‰«æ Excel æ–‡ä»¶
   */
  async scoutExcelFile(
    filePath: string,
    options: {
      sampleRows?: number;      // é‡‡æ ·è¡Œæ•°,é»˜è®¤3
      detectPatterns?: boolean;  // æ˜¯å¦æ£€æµ‹æ¨¡å¼
      analyzeQuality?: boolean;  // æ˜¯å¦åˆ†æè´¨é‡
    } = {}
  ): Promise<ExcelScoutReport> {
    const {
      sampleRows = 3,
      detectPatterns = true,
      analyzeQuality = true
    } = options;

    // æ£€æŸ¥ç¼“å­˜
    const cacheKey = `excel_scout_${filePath}_${sampleRows}`;
    const cached = await this.cacheService.get(cacheKey);
    if (cached) return cached;

    // æ‰§è¡Œä¾¦å¯Ÿè„šæœ¬
    const script = this.buildScoutScript(filePath, {
      sampleRows,
      detectPatterns,
      analyzeQuality
    });

    const result = await this.pyodideService.runPython(script);

    // è§£æç»“æœ
    const report: ExcelScoutReport = JSON.parse(result);

    // ç¼“å­˜ç»“æœ
    await this.cacheService.set(cacheKey, report, { ttl: 3600 });

    return report;
  }

  /**
   * æ„å»ºä¾¦å¯Ÿè„šæœ¬
   */
  private buildScoutScript(
    filePath: string,
    options: ScoutOptions
  ): string {
    return `
import pandas as pd
import json
import re
from datetime import datetime

def scout_excel(path, sample_rows=3, detect_patterns=True, analyze_quality=True):
    """æ·±åº¦æ‰«æ Excel æ–‡ä»¶"""

    report = {
        "filename": "${filePath}",
        "scan_time": datetime.now().isoformat(),
        "sheets": [],
        "global_stats": {
            "total_sheets": 0,
            "total_estimated_rows": 0,
            "has_errors": False
        },
        "patterns": {},
        "quality_issues": []
    }

    try:
        xl = pd.ExcelFile(path)
        report["global_stats"]["total_sheets"] = len(xl.sheet_names)

        for sheet_name in xl.sheet_names:
            sheet_report = {
                "name": sheet_name,
                "columns": {},
                "sample_data": [],
                "stats": {}
            }

            # è¯»å–æ ·æœ¬æ•°æ®
            df = pd.read_excel(path, sheet_name=sheet_name, nrows=sample_rows)

            # ä¼°ç®—æ€»è¡Œæ•° (è¯»å–å‰1000è¡Œè¿›è¡Œä¼°ç®—)
            try:
                df_full = pd.read_excel(path, sheet_name=sheet_name, nrows=1000)
                report["global_stats"]["total_estimated_rows"] += len(df_full)
                if len(df_full) == 1000:
                    # å¦‚æœè¾¾åˆ°äº†1000è¡Œ,è¯´æ˜å®é™…æ•°æ®æ›´å¤š
                    report["global_stats"]["total_estimated_rows"] += "*"
            except:
                pass

            # åˆ†ææ¯ä¸€åˆ—
            for col in df.columns:
                col_info = {
                    "name": col,
                    "dtype": str(df[col].dtype),
                    "null_count": int(df[col].isna().sum()),
                    "null_ratio": float(df[col].isna().sum() / len(df)),
                    "sample_values": []
                }

                # æå–éç©ºæ ·ä¾‹
                non_null = df[col].dropna()
                for val in non_null.head(min(3, len(non_null))):
                    col_info["sample_values"].append(str(val))

                # æ¨¡å¼æ£€æµ‹
                if detect_patterns:
                    patterns = detect_column_patterns(df[col])
                    col_info["patterns"] = patterns

                # è´¨é‡åˆ†æ
                if analyze_quality:
                    issues = analyze_column_quality(df[col])
                    if issues:
                        col_info["quality_issues"] = issues
                        report["quality_issues"].extend([
                            { "sheet": sheet_name, "column": col, "issue": issue }
                            for issue in issues
                        ])

                sheet_report["columns"][col] = col_info

            # ä¿å­˜æ ·æœ¬æ•°æ®
            sheet_report["sample_data"] = df.head(sample_rows).to_dict('records')

            # ç»Ÿè®¡ä¿¡æ¯
            sheet_report["stats"] = {
                "row_count_sample": len(df),
                "column_count": len(df.columns),
                "has_nulls": df.isna().any().any()
            }

            report["sheets"].append(sheet_report)

        # å…¨å±€æ¨¡å¼æ€»ç»“
        if detect_patterns:
            report["patterns"] = summarize_global_patterns(report["sheets"])

    except Exception as e:
        report["error"] = str(e)
        report["global_stats"]["has_errors"] = True

    return json.dumps(report, ensure_ascii=False)

def detect_column_patterns(series):
    """æ£€æµ‹åˆ—ä¸­çš„æ•°æ®æ¨¡å¼"""
    patterns = {
        "has_thousands_separator": False,
        "has_currency_symbol": False,
        "has_percentage": False,
        "has_date_format": False,
        "is_serial_date": False,
        "empty_string_ratio": 0
    }

    non_null = series.dropna()
    if len(non_null) == 0:
        return patterns

    sample = non_null.head(10)
    sample_str = sample.astype(str)

    # æ£€æµ‹åƒåˆ†ä½
    patterns["has_thousands_separator"] = sample_str.str.contains(',').any()

    # æ£€æµ‹è´§å¸ç¬¦å·
    patterns["has_currency_symbol"] = sample_str.str.contains('[$Â¥â‚¬Â£]').any()

    # æ£€æµ‹ç™¾åˆ†æ¯”
    patterns["has_percentage"] = sample_str.str.contains('%').any()

    # æ£€æµ‹æ—¥æœŸæ ¼å¼
    date_patterns = [
        r'\\d{4}-\\d{2}-\\d{2}',
        r'\\d{2}/\\d{2}/\\d{4}',
        r'\\d{4}å¹´\\d{1,2}æœˆ\\d{1,2}æ—¥'
    ]
    patterns["has_date_format"] = any(
        sample_str.str.contains(pat, regex=True).any()
        for pat in date_patterns
    )

    # æ£€æµ‹ Serial Date (Excel çš„æ•°å­—æ—¥æœŸ)
    if series.dtype in ['int64', 'float64']:
        numeric_vals = series.dropna()
        if len(numeric_vals) > 0:
            # Excel æ—¥æœŸèŒƒå›´å¤§çº¦åœ¨ 1-60000 ä¹‹é—´ (1900-2064)
            patterns["is_serial_date"] = (
                numeric_vals.between(1, 60000).all() and
                numeric_vals.max() > 30000  # æ’é™¤æ™®é€šæ•°å­—
            )

    # æ£€æµ‹ç©ºå­—ç¬¦ä¸²
    patterns["empty_string_ratio"] = (sample_str == '').sum() / len(sample_str)

    return patterns

def analyze_column_quality(series):
    """åˆ†æåˆ—çš„æ•°æ®è´¨é‡"""
    issues = []

    # ç©ºå€¼ç‡è¿‡é«˜
    null_ratio = series.isna().sum() / len(series)
    if null_ratio > 0.5:
        issues.append({
            "type": "high_null_ratio",
            "severity": "warning",
            "value": null_ratio
        })

    # æ•°æ®ç±»å‹ä¸ä¸€è‡´
    if series.dtype == 'object':
        non_null = series.dropna()
        if len(non_null) > 0:
            # æ£€æµ‹æ˜¯å¦æœ‰æ··åˆç±»å‹
            types = non_null.apply(type).nunique()
            if types > 2:
                issues.append({
                    "type": "mixed_types",
                    "severity": "warning",
                    "value": types
                })

    return issues

def summarize_global_patterns(sheets):
    """æ€»ç»“å…¨å±€æ¨¡å¼"""
    return {
        "common_patterns": ["åƒåˆ†ä½", "æ—¥æœŸæ ¼å¼"],
        "recommendations": [
            "å»ºè®®åœ¨ä½¿ç”¨å‰æ¸…æ´—é‡‘é¢åˆ—",
            "æ—¥æœŸåˆ—å¯èƒ½éœ€è¦ç‰¹æ®Šè½¬æ¢"
        ]
    }

# æ‰§è¡Œæ‰«æ
result = scout_excel(
    "${filePath}",
    sample_rows=${options.sampleRows},
    detect_patterns=${options.detectPatterns},
    analyze_quality=${options.analyzeQuality}
)
result
    `;
  }
}
```

#### Word ä¾¦å¯Ÿå…µ

**é¡¾é—®æä¾›çš„æ–¹æ¡ˆ**:
```python
from docx import Document
import json

def scout_document_structure(file_path):
    doc = Document(file_path)
    structure = []
    table_index = 0

    for i, element in enumerate(doc.element.body):
        if element.tag.endswith('p'):
            para = Paragraph(element, doc)
            text = para.text.strip()
            if text and ("{{" in text or len(text) < 50):
                structure.append({
                    "type": "paragraph",
                    "index": i,
                    "text": text,
                    "has_slot": "{{" in text
                })

        elif element.tag.endswith('tbl'):
            table = Table(element, doc)
            headers = [cell.text.strip() for cell in table.rows[0].cells]

            structure.append({
                "type": "table",
                "table_index": table_index,
                "global_index": i,
                "rows": len(table.rows),
                "cols": len(table.columns),
                "headers": headers,
                "sample_row": [cell.text.strip() for cell in table.rows[1].cells] if len(table.rows) > 1 else []
            })
            table_index += 1

    return json.dumps(structure, ensure_ascii=False)
```

#### å½“å‰ç³»ç»ŸçŠ¶æ€

**å·²å®ç°** âœ…:
- åŸºç¡€ Word æ–‡æ¡£è¯»å– (`docxtemplaterService`)
- æ®µè½å’Œè¡¨æ ¼æå–

**ç¼ºå¤±** âš ï¸:
- å ä½ç¬¦è¯†åˆ«
- è¡¨æ ¼ç»“æ„æ·±åº¦åˆ†æ
- æ ·å¼ä¿¡æ¯æå–
- ä½ç½®ç´¢å¼•

#### å¢å¼ºå®ç°

```typescript
/**
 * Word æ·±åº¦ä¾¦å¯ŸæœåŠ¡
 */
class WordScoutService {
  async scoutWordDocument(
    file: File
  ): Promise<WordScoutReport> {
    // ä½¿ç”¨ docxtemplater çš„åŸºç¡€åŠŸèƒ½ + è‡ªå®šä¹‰åˆ†æ

    const report: WordScoutReport = {
      filename: file.name,
      structure: [],
      placeholders: [],
      tables: [],
      styles: [],
      statistics: {}
    };

    // 1. è§£ææ–‡æ¡£ç»“æ„
    const structure = await this.parseDocumentStructure(file);

    // 2. è¯†åˆ«å ä½ç¬¦
    const placeholders = await this.identifyPlaceholders(structure);

    // 3. åˆ†æè¡¨æ ¼
    const tables = await this.analyzeTables(structure);

    // 4. æå–æ ·å¼
    const styles = await this.extractStyles(file);

    return {
      ...report,
      structure,
      placeholders,
      tables,
      styles
    };
  }

  /**
   * è¯†åˆ«å ä½ç¬¦
   */
  private async identifyPlaceholders(
    structure: DocumentStructure
  ): Promise<Placeholder[]> {
    const placeholders: Placeholder[] = [];

    // æ‰«ææ®µè½
    structure.paragraphs.forEach((para, index) => {
      const matches = para.text.match(/\{\{[^}]+\}\}/g);
      if (matches) {
        matches.forEach(match => {
          placeholders.push({
            key: match.replace(/\{\{|\}\}/g, ''),
            type: this.inferPlaceholderType(match),
            location: { type: 'paragraph', index },
            context: this.extractContext(para, 20)
          });
        });
      }
    });

    // æ‰«æè¡¨æ ¼å•å…ƒæ ¼
    structure.tables.forEach((table, tableIndex) => {
      table.rows.forEach((row, rowIndex) => {
        row.cells.forEach((cell, cellIndex) => {
          const matches = cell.text.match(/\{\{[^}]+\}\}/g);
          if (matches) {
            matches.forEach(match => {
              placeholders.push({
                key: match.replace(/\{\{|\}\}/g, ''),
                type: this.inferPlaceholderType(match),
                location: {
                  type: 'table',
                  tableIndex,
                  rowIndex,
                  cellIndex
                },
                context: this.extractContext(cell, 20)
              });
            });
          }
        });
      });
    });

    return placeholders;
  }

  /**
   * æ¨æ–­å ä½ç¬¦ç±»å‹
   */
  private inferPlaceholderType(placeholder: string): PlaceholderType {
    const key = placeholder.replace(/\{\{|\}\}/g, '').toLowerCase();

    if (key.includes('åˆ—è¡¨') || key.includes('æ˜ç»†') || key.includes('items')) {
      return 'loop';
    }

    if (key.includes('å›¾ç‰‡') || key.includes('image')) {
      return 'image';
    }

    if (key.includes('æ¡ä»¶') || key.includes('if')) {
      return 'condition';
    }

    return 'simple';
  }

  /**
   * åˆ†æè¡¨æ ¼
   */
  private async analyzeTables(
    structure: DocumentStructure
  ): Promise<TableAnalysis[]> {
    return structure.tables.map((table, index) => ({
      index,
      globalIndex: table.globalIndex,
      dimensions: {
        rows: table.rowCount,
        cols: table.colCount
      },
      headers: this.extractTableHeaders(table),
      sampleData: this.extractTableSample(table),
      hasMergedCells: this.detectMergedCells(table),
      isEmpty: table.rowCount === 0,
      likelyPurpose: this.inferTablePurpose(table)
    }));
  }

  /**
   * æ¨æ–­è¡¨æ ¼ç”¨é€”
   */
  private inferTablePurpose(table: TableInfo): string {
    const headers = table.rows[0].cells.map(c => c.text.toLowerCase());

    if (headers.some(h => h.includes('é‡‘é¢') || h.includes('æ•°é‡'))) {
      return 'data_table';
    }

    if (headers.some(h => h.includes('ç­¾å') || h.includes('æ—¥æœŸ'))) {
      return 'form_table';
    }

    if (table.rowCount > 10) {
      return 'detail_table';
    }

    return 'unknown';
  }
}
```

#### å®æ–½ä¼˜å…ˆçº§: **P0 (åŸºç¡€èƒ½åŠ›)**

---

### 2.2 é¢„å®¡å¼•æ“ (Pre-Filter Engine)

#### é¡¾é—®æä¾›çš„å¼‚å¸¸é¢„å®¡è„šæœ¬

**æ ¸å¿ƒåŠŸèƒ½**:
1. æ ¹æ®å†…æ§è§„åˆ™è¿‡æ»¤ Excel æ•°æ®
2. æ”¯æŒå¤šç§æ¯”è¾ƒè¿ç®—ç¬¦
3. è®°å½•è¿è§„è¡Œå·å’Œè¯æ®
4. ç”Ÿæˆç»“æ„åŒ–å¼‚å¸¸æŠ¥å‘Š

#### æ¶æ„è®¾è®¡

```typescript
/**
 * å†…æ§é¢„å®¡å¼•æ“
 *
 * èŒè´£:
 * 1. è§„åˆ™è§£æå’ŒéªŒè¯
 * 2. æ•°æ®ç­›é€‰å’Œå¼‚å¸¸æ£€æµ‹
 * 3. é£é™©è¯„åˆ†
 * 4. è¯æ®é“¾ç”Ÿæˆ
 */
class InternalControlPreFilterEngine {
  constructor(
    private pyodideService: PyodideService,
    private ruleEngine: RuleEngine
  ) {}

  /**
   * æ‰§è¡Œé¢„å®¡
   */
  async executePreFilter(
    dataSource: DataSourceInfo[],
    controlRules: InternalControlRule[]
  ): Promise<PreFilterResult> {
    // 1. éªŒè¯è§„åˆ™
    const validatedRules = await this.ruleEngine.validateRules(controlRules);

    // 2. æ„å»ºç­›é€‰è„šæœ¬
    const filterScript = this.buildFilterScript(dataSource, validatedRules);

    // 3. åœ¨ WASM ä¸­æ‰§è¡Œ
    const rawResult = await this.pyodideService.runPython(filterScript);

    // 4. è§£æç»“æœ
    const exceptions: ExceptionRecord[] = JSON.parse(rawResult);

    // 5. é£é™©è¯„åˆ†
    const riskAssessment = await this.assessRisks(exceptions, validatedRules);

    return {
      exceptions,
      riskAssessment,
      summary: this.buildSummary(exceptions, riskAssessment),
      executionTime: 0  // TODO: æ·»åŠ è®¡æ—¶
    };
  }

  /**
   * æ„å»ºç­›é€‰è„šæœ¬
   */
  private buildFilterScript(
    dataSources: DataSourceInfo[],
    rules: InternalControlRule[]
  ): string {
    return `
import pandas as pd
import json
import operator

# æ”¯æŒçš„è¿ç®—ç¬¦
ops = {
    ">": operator.gt,
    "<": operator.lt,
    ">=": operator.ge,
    "<=": operator.le,
    "==": operator.eq,
    "!=": operator.ne,
    "contains": lambda x, y: y.lower() in str(x).lower(),
    "not_contains": lambda x, y: y.lower() not in str(x).lower()
}

def run_pre_filter(data_sources, rules):
    """æ‰§è¡Œé¢„å®¡ç­›é€‰"""
    all_exceptions = []

    for source in data_sources:
        file_path = source['path']
        file_name = source['name']

        try:
            # è¯»å–æ•°æ®
            df = pd.read_excel(file_path)

            # å¯¹æ¯æ¡è§„åˆ™æ‰§è¡Œæ£€æŸ¥
            for rule in rules:
                col = rule['column']
                op_str = rule['operator']
                val = rule['value']

                # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                if col not in df.columns:
                    continue

                # æ‰§è¡Œç­›é€‰
                try:
                    # ç±»å‹è½¬æ¢
                    if isinstance(val, (int, float)):
                        df[col] = pd.to_numeric(df[col], errors='coerce')

                    # åº”ç”¨æ¡ä»¶
                    mask = ops[op_str](df[col], val)
                    exceptions_df = df[mask].copy()

                    if not exceptions_df.empty:
                        # è®°å½•å¼‚å¸¸
                        for idx, row in exceptions_df.iterrows():
                            exception = {
                                "rule_id": rule['id'],
                                "rule_description": rule['description'],
                                "source_file": file_name,
                                "row_number": int(idx + 2),  # Excel è¡Œå·
                                "column": col,
                                "actual_value": str(row[col]),
                                "expected_value": str(val),
                                "operator": op_str,
                                "evidence": row.to_dict(),
                                "severity": rule.get('severity', 'medium')
                            }
                            all_exceptions.append(exception)

                except Exception as e:
                    # è®°å½•è§„åˆ™æ‰§è¡Œå¤±è´¥
                    all_exceptions.append({
                        "rule_id": rule['id'],
                        "error": str(e),
                        "severity": "low"
                    })

        except Exception as e:
            all_exceptions.append({
                "source_file": file_name,
                "error": str(e),
                "severity": "low"
            })

    return json.dumps(all_exceptions, ensure_ascii=False)

# æ‰§è¡Œ
data_sources = ${JSON.stringify(dataSources)}
rules = ${JSON.stringify(rules)}

result = run_pre_filter(data_sources, rules)
result
    `;
  }

  /**
   * é£é™©è¯„ä¼°
   */
  private async assessRisks(
    exceptions: ExceptionRecord[],
    rules: InternalControlRule[]
  ): Promise<RiskAssessment> {
    // è®¡ç®—é£é™©è¯„åˆ†
    const highRiskCount = exceptions.filter(e => e.severity === 'high').length;
    const mediumRiskCount = exceptions.filter(e => e.severity === 'medium').length;
    const lowRiskCount = exceptions.filter(e => e.severity === 'low').length;

    // åŠ æƒè¯„åˆ†
    const riskScore = highRiskCount * 10 + mediumRiskCount * 5 + lowRiskCount * 1;

    // é£é™©ç­‰çº§
    let riskLevel: 'low' | 'medium' | 'high' | 'critical';
    if (riskScore > 100) {
      riskLevel = 'critical';
    } else if (riskScore > 50) {
      riskLevel = 'high';
    } else if (riskScore > 20) {
      riskLevel = 'medium';
    } else {
      riskLevel = 'low';
    }

    return {
      score: riskScore,
      level: riskLevel,
      breakdown: {
        high: highRiskCount,
        medium: mediumRiskCount,
        low: lowRiskCount
      },
      recommendations: this.generateRiskRecommendations(exceptions, riskLevel)
    };
  }

  /**
   * ç”Ÿæˆé£é™©å»ºè®®
   */
  private generateRiskRecommendations(
    exceptions: ExceptionRecord[],
    riskLevel: string
  ): string[] {
    const recommendations: string[] = [];

    if (riskLevel === 'critical') {
      recommendations.push('å‘ç°é‡å¤§å†…æ§ç¼ºé™·,å»ºè®®ç«‹å³åœæ­¢ç›¸å…³ä¸šåŠ¡å¹¶è¿›è¡Œå…¨é¢å®¡æŸ¥');
    }

    // æŒ‰è§„åˆ™åˆ†ç»„
    const byRule = this.groupBy(exceptions, 'rule_id');
    for (const [ruleId, excs] of Object.entries(byRule)) {
      if (excs.length > 10) {
        recommendations.push(`è§„åˆ™ "${excs[0].rule_description}" è¿è§„æ¬¡æ•°è¾¾ ${excs.length} æ¬¡,éœ€é‡ç‚¹æ ¸æŸ¥`);
      }
    }

    return recommendations;
  }
}
```

#### å®æ–½ä¼˜å…ˆçº§: **P1 (é«˜çº§åŠŸèƒ½)**

ç†ç”±:
1. æ˜¯"å†…æ§è¯„ä»·æ¨¡å¼"çš„æ ¸å¿ƒ
2. æ¶æ„ç‹¬ç«‹,å¯åç»­æ·»åŠ 
3. ä¸šåŠ¡ä»·å€¼é«˜

---

### 2.3 å¡«å……å™¨ (Document Filler)

#### é¡¾é—®æä¾›çš„ DataFrame è½¬ Word è¡¨æ ¼æ–¹æ¡ˆ

**æ ¸å¿ƒäº®ç‚¹**:
1. **æ ·å¼å…‹éš†** - ä¿ç•™åŸå§‹æ ¼å¼
2. **åŠ¨æ€æ‰©å®¹** - è‡ªåŠ¨å¢åˆ è¡Œ
3. **Run çº§æ“ä½œ** - ç²¾ç¡®æ§åˆ¶æ ¼å¼
4. **åˆå¹¶å•å…ƒæ ¼æ”¯æŒ**

#### å½“å‰ç³»ç»Ÿå¯¹æ¯”

| åŠŸèƒ½ | é¡¾é—®æ–¹æ¡ˆ | docxtemplaterService | å·®è· |
|------|---------|---------------------|------|
| åŸºç¡€å¡«å…… | âœ… | âœ… | æ—  |
| æ ·å¼ä¿æŒ | âœ… æ‰‹åŠ¨å…‹éš† | âœ… è‡ªåŠ¨ | **æ–¹æ¡ˆä¸åŒ** |
| åŠ¨æ€è¡¨æ ¼ | âœ… | âœ… | æ—  |
| æ ¼å¼åŒ– | âœ… è‡ªå®šä¹‰ | âš ï¸ æœ‰é™ | **éœ€å¢å¼º** |
| åˆå¹¶å•å…ƒæ ¼ | âœ… æ”¯æŒ | âš ï¸ éƒ¨åˆ† | **éœ€éªŒè¯** |

#### æ¶æ„å†³ç­–: ä¿æŒ docxtemplater ä½œä¸ºä¸»å¼•æ“

**ç†ç”±**:
1. âœ… å·²æœ‰æˆç†Ÿå®ç°
2. âœ… æ ¼å¼ä¿æŒç‡ 95-98%
3. âœ… æ”¯æŒå¤æ‚ç‰¹æ€§(å¾ªç¯ã€æ¡ä»¶ã€å›¾ç‰‡)
4. âœ… æ€§èƒ½ä¼˜ç§€

**å¢å¼ºç­–ç•¥**:
```typescript
/**
 * å¢å¼ºçš„æ–‡æ¡£å¡«å……æœåŠ¡
 * èåˆé¡¾é—®çš„æœ€ä½³å®è·µ
 */
class EnhancedDocumentFiller {
  /**
   * æ™ºèƒ½å¡«å…… - æ ¹æ®æ•°æ®ç±»å‹è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
   */
  async smartFill(
    template: File,
    data: MappingData,
    options: FillOptions
  ): Promise<FilledDocument> {
    // 1. åˆ†ææ¨¡æ¿å¤æ‚åº¦
    const complexity = await this.analyzeTemplateComplexity(template);

    // 2. é€‰æ‹©å¡«å……ç­–ç•¥
    const strategy = this.selectFillStrategy(complexity, data);

    // 3. æ•°æ®é¢„å¤„ç†
    const processedData = await this.preprocessData(data, strategy);

    // 4. æ‰§è¡Œå¡«å……
    const result = await strategy.fill(template, processedData, options);

    // 5. åå¤„ç†éªŒè¯
    await this.postProcessValidation(result);

    return result;
  }

  /**
   * é€‰æ‹©å¡«å……ç­–ç•¥
   */
  private selectFillStrategy(
    complexity: TemplateComplexity,
    data: MappingData
  ): FillStrategy {
    // ç®€å•æ¨¡æ¿ â†’ docxtemplater
    if (complexity.level === 'simple') {
      return new DocxtemplaterFillStrategy();
    }

    // å¤æ‚è¡¨æ ¼ â†’ python-docx
    if (complexity.hasComplexTables && data.hasLargeDataFrames) {
      return new PythonDocxFillStrategy();
    }

    // æ··åˆåœºæ™¯ â†’ ç»„åˆç­–ç•¥
    return new HybridFillStrategy();
  }
}

/**
 * Python-docx å¡«å……ç­–ç•¥
 * å®ç°é¡¾é—®çš„ç²¾ç¡®æ§åˆ¶æ–¹æ¡ˆ
 */
class PythonDocxFillStrategy implements FillStrategy {
  async fill(
    template: File,
    data: MappingData,
    options: FillOptions
  ): Promise<Blob> {
    // ä½¿ç”¨ WASM æ‰§è¡Œ python-docx è„šæœ¬
    const script = this.buildPythonDocxScript(template, data, options);

    const result = await this.pyodideService.runPython(script);

    return this.convertToBlob(result);
  }

  private buildPythonDocxScript(
    template: File,
    data: MappingData,
    options: FillOptions
  ): string {
    return `
from docx import Document
from docx.shared import Pt
import pandas as pd

def fill_table_with_df(doc_path, table_index, df, start_row=1):
    """å°† DataFrame å¡«å…¥ Word è¡¨æ ¼,ä¿æŒæ ¼å¼"""
    doc = Document(doc_path)
    table = doc.tables[table_index]

    # è·å–æ ·æ¿è¡Œ
    sample_row_cells = table.rows[start_row].cells

    # æ‰©å®¹
    rows_needed = len(df)
    existing_rows = len(table.rows) - start_row

    if rows_needed > existing_rows:
        for _ in range(rows_needed - existing_rows):
            table.add_row()

    # å¡«å……æ•°æ®
    for i, (idx, row_data) in enumerate(df.iterrows()):
        current_row = table.rows[start_row + i]

        for j, value in enumerate(row_data):
            if j < len(current_row.cells):
                cell = current_row.cells[j]

                # æ¸…é™¤ä½†ä¿ç•™æ ¼å¼
                paragraph = cell.paragraphs[0]
                paragraph.text = str(value)

                # æ ·å¼è¡¥å¿
                if i > 0:
                    sample_paragraph = sample_row_cells[j].paragraphs[0]
                    if sample_paragraph.runs:
                        sample_run = sample_paragraph.runs[0]
                        for run in paragraph.runs:
                            run.font.name = sample_run.font.name
                            run.font.size = sample_run.font.size

    return doc

# æ‰§è¡Œå¡«å……
${this.generateExecutionCode(data, options)}
    `;
  }
}
```

#### å®æ–½ä¼˜å…ˆçº§: **P2 (ä¼˜åŒ–é¡¹)**

ç†ç”±:
1. ç°æœ‰æ–¹æ¡ˆå·²åŸºæœ¬æ»¡è¶³éœ€æ±‚
2. æ–°å¢ç­–ç•¥å¯ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
3. ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½

---

### 2.4 è‡ªæ„ˆé€»è¾‘ (Self-Healing)

#### é¡¾é—®æä¾›çš„è‡ªæ„ˆå¾ªç¯è®¾è®¡

**æ ¸å¿ƒæœºåˆ¶**:
1. æ•è· Python Traceback
2. åé¦ˆç»™ AI è¯·æ±‚ä¿®å¤
3. é‡æ–°æ‰§è¡Œ
4. æœ€å¤šé‡è¯• 3 æ¬¡

#### å½“å‰ç³»ç»Ÿå®ç°

**å·²å®ç°** âœ…:
```typescript
// AgenticOrchestrator.ts
private async handleError(error: TaskError): Promise<RepairResult> {
  // ... é”™è¯¯å¤„ç†é€»è¾‘
}
```

**å·®è·**:
- âŒ ç¼ºå°‘è¯¦ç»†çš„é”™è¯¯åˆ†ç±»
- âŒ æœªå……åˆ†åˆ©ç”¨é”™è¯¯ä¿¡æ¯
- âš ï¸ é‡è¯•ç­–ç•¥è¾ƒç®€å•

#### å¢å¼ºæ–¹æ¡ˆ

```typescript
/**
 * å¢å¼ºçš„è‡ªæ„ˆå¼•æ“
 */
class SelfHealingEngine {
  /**
   * æ™ºèƒ½é”™è¯¯å¤„ç†
   */
  async handleExecutionError(
    error: Error,
    failedCode: string,
    context: ExecutionContext
  ): Promise<HealingResult> {
    // 1. é”™è¯¯åˆ†ç±»
    const errorCategory = this.classifyError(error);

    // 2. å†³å®šä¿®å¤ç­–ç•¥
    const strategy = this.selectHealingStrategy(errorCategory);

    // 3. æ‰§è¡Œä¿®å¤
    const result = await strategy.execute(error, failedCode, context);

    return result;
  }

  /**
   * é”™è¯¯åˆ†ç±»
   */
  private classifyError(error: Error): ErrorCategory {
    const message = error.message.toLowerCase();

    // KeyError â†’ åˆ—åé—®é¢˜
    if (message.includes('keyerror') || message.includes('key')) {
      return {
        type: 'ColumnNotFound',
        severity: 'medium',
        autoFixable: true,
        suggestedFix: 'æ£€æŸ¥åˆ—åæ‹¼å†™,å°è¯•å»é™¤ç©ºæ ¼'
      };
    }

    // TypeError â†’ ç±»å‹ä¸åŒ¹é…
    if (message.includes('typeerror') || message.includes('type')) {
      return {
        type: 'TypeMismatch',
        severity: 'medium',
        autoFixable: true,
        suggestedFix: 'æ·»åŠ ç±»å‹è½¬æ¢: pd.to_numeric()'
      };
    }

    // MergeError â†’ å…³è”å¤±è´¥
    if (message.includes('merge') || message.includes('join')) {
      return {
        type: 'MergeFailed',
        severity: 'high',
        autoFixable: false,
        suggestedFix: 'æ£€æŸ¥å…³è”é”®æ˜¯å¦å­˜åœ¨äºä¸¤ä¸ªè¡¨ä¸­'
      };
    }

    // é»˜è®¤
    return {
      type: 'Unknown',
      severity: 'low',
      autoFixable: false,
      suggestedFix: 'éœ€è¦äººå·¥ä»‹å…¥'
    };
  }

  /**
   * é€‰æ‹©ä¿®å¤ç­–ç•¥
   */
  private selectHealingStrategy(category: ErrorCategory): HealingStrategy {
    if (!category.autoFixable) {
      return new ManualInterventionStrategy();
    }

    switch (category.type) {
      case 'ColumnNotFound':
        return new ColumnNameFixStrategy();
      case 'TypeMismatch':
        return new TypeConversionStrategy();
      case 'MergeFailed':
        return new MergeRetryStrategy();
      default:
        return new GenericRetryStrategy();
    }
  }
}

/**
 * åˆ—åä¿®å¤ç­–ç•¥
 */
class ColumnNameFixStrategy implements HealingStrategy {
  async execute(
    error: Error,
    failedCode: string,
    context: ExecutionContext
  ): Promise<HealingResult> {
    // 1. æå–ç¼ºå¤±çš„åˆ—å
    const missingColumn = this.extractMissingColumn(error.message);

    // 2. ä»å…ƒæ•°æ®ä¸­æŸ¥æ‰¾ç›¸ä¼¼åˆ—å
    const suggestions = await this.findSimilarColumns(
      missingColumn,
      context.metadata
    );

    // 3. æ„å»ºä¿®å¤ Prompt
    const repairPrompt = this.buildRepairPrompt({
      error: error.message,
      missingColumn,
      suggestions,
      originalCode: failedCode
    });

    // 4. è¯·æ±‚ AI ä¿®å¤
    const fixedCode = await context.aiService.fixCode(repairPrompt);

    // 5. éªŒè¯ä¿®å¤
    const isValid = await this.validateFix(fixedCode, context);

    return {
      success: isValid,
      fixedCode,
      appliedFix: `å°† "${missingColumn}" æ›¿æ¢ä¸º "${suggestions[0]}"`
    };
  }

  private async findSimilarColumns(
    target: string,
    metadata: FileMetadata
  ): Promise<string[]> {
    // ä½¿ç”¨å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ç®—æ³•
    const columns = Object.keys(metadata.columns);
    const similarities = columns.map(col => ({
      column: col,
      score: this.calculateSimilarity(target, col)
    }));

    return similarities
      .filter(s => s.score > 0.6)
      .sort((a, b) => b.score - a.score)
      .slice(0, 3)
      .map(s => s.column);
  }

  private calculateSimilarity(str1: string, str2: string): number {
    // Levenshtein è·ç¦»ç®—æ³•
    // ...
  }
}
```

#### å®æ–½ä¼˜å…ˆçº§: **P0 (å¯é æ€§)**

ç†ç”±:
1. ç›´æ¥å½±å“ç”¨æˆ·ä½“éªŒ
2. å¤§å¹…å‡å°‘äººå·¥ä»‹å…¥
3. æå‡ç³»ç»Ÿæ™ºèƒ½åŒ–æ°´å¹³

---

### 2.5 é€‚é…å™¨ (Function Calling Adapter)

#### é¡¾é—®æå‡ºçš„æ™ºèƒ½ä½“é›†æˆæ–¹æ¡ˆ

**æ ¸å¿ƒæ€è·¯**:
- å°† Chatbot ä»"å¯¹è¯å·¥å…·"å‡çº§ä¸º"æ‰§è¡ŒåŠ©æ‰‹"
- ä½¿ç”¨ Function Calling è®© AI ä¸»åŠ¨è°ƒç”¨å·¥å…·
- å®ç°è‡ªç„¶è¯­è¨€ â†’ å·¥å…·è°ƒç”¨ â†’ ç»“æœåé¦ˆçš„é—­ç¯

#### æ¶æ„è®¾è®¡

```typescript
/**
 * Function Calling é€‚é…å™¨
 * è¿æ¥ AI æ™ºèƒ½ä½“ä¸ç³»ç»Ÿå·¥å…·
 */
class FunctionCallingAdapter {
  private tools: ToolRegistry;

  constructor() {
    this.tools = new ToolRegistry();
    this.registerDefaultTools();
  }

  /**
   * æ³¨å†Œé»˜è®¤å·¥å…·
   */
  private registerDefaultTools() {
    // Excel åˆ†æå·¥å…·
    this.tools.register({
      name: 'analyze_excel_structure',
      description: 'åˆ†æ Excel æ–‡ä»¶çš„ç»“æ„,æå–è¡¨å¤´å’Œæ•°æ®ç±»å‹',
      parameters: {
        type: 'object',
        properties: {
          filePath: {
            type: 'string',
            description: 'Excel æ–‡ä»¶è·¯å¾„'
          }
        },
        required: ['filePath']
      },
      handler: async (args) => {
        return await this.excelScout.scoutExcelFile(args.filePath);
      }
    });

    // æ•°æ®æŸ¥è¯¢å·¥å…·
    this.tools.register({
      name: 'query_data',
      description: 'ä» Excel ä¸­æŸ¥è¯¢ç¬¦åˆæ¡ä»¶çš„æ•°æ®',
      parameters: {
        type: 'object',
        properties: {
          filePath: { type: 'string' },
          filters: { type: 'array' },
          columns: { type: 'array' }
        },
        required: ['filePath', 'filters']
      },
      handler: async (args) => {
        return await this.queryEngine.query(args);
      }
    });

    // å¼‚å¸¸æ£€æµ‹å·¥å…·
    this.tools.register({
      name: 'detect_anomalies',
      description: 'æ ¹æ®è§„åˆ™æ£€æµ‹æ•°æ®ä¸­çš„å¼‚å¸¸',
      parameters: {
        type: 'object',
        properties: {
          filePath: { type: 'string' },
          rules: { type: 'array' }
        },
        required: ['filePath', 'rules']
      },
      handler: async (args) => {
        return await this.preFilterEngine.execute(args);
      }
    });

    // æ–‡æ¡£å¡«å……å·¥å…·
    this.tools.register({
      name: 'fill_document',
      description: 'å°†æ•°æ®å¡«å……åˆ° Word æ¨¡æ¿ä¸­',
      parameters: {
        type: 'object',
        properties: {
          templatePath: { type: 'string' },
          data: { type: 'object' },
          outputPath: { type: 'string' }
        },
        required: ['templatePath', 'data']
      },
      handler: async (args) => {
        return await this.documentFiller.fill(args);
      }
    });
  }

  /**
   * å¤„ç† AI çš„å·¥å…·è°ƒç”¨è¯·æ±‚
   */
  async handleToolCalls(
    toolCalls: ToolCall[],
    context: ConversationContext
  ): Promise<ToolCallResult[]> {
    const results: ToolCallResult[] = [];

    for (const call of toolCalls) {
      const tool = this.tools.get(call.name);

      if (!tool) {
        results.push({
          id: call.id,
          success: false,
          error: `Unknown tool: ${call.name}`
        });
        continue;
      }

      try {
        // æ‰§è¡Œå·¥å…·
        const output = await tool.handler(call.arguments, context);

        results.push({
          id: call.id,
          success: true,
          output
        });

      } catch (error) {
        results.push({
          id: call.id,
          success: false,
          error: error.message
        });
      }
    }

    return results;
  }

  /**
   * æ„å»ºå·¥å…·å®šä¹‰ (ç”¨äº AI API)
   */
  buildToolsDefinition(): ToolDefinition[] {
    return Array.from(this.tools.getAll()).map(([name, tool]) => ({
      name,
      description: tool.description,
      parameters: tool.parameters
    }));
  }
}

/**
 * æ™ºèƒ½å¯¹è¯æœåŠ¡
 * é›†æˆ Function Calling
 */
class IntelligentChatService {
  constructor(
    private aiService: AIService,
    private toolAdapter: FunctionCallingAdapter
  ) {}

  /**
   * å¤„ç†ç”¨æˆ·æ¶ˆæ¯
   */
  async processUserMessage(
    message: string,
    context: ConversationContext
  ): Promise<ChatResponse> {
    // 1. æ„å»ºå¯¹è¯è¯·æ±‚
    const request = {
      model: 'glm-4.6',
      messages: [
        ...context.history,
        { role: 'user', content: message }
      ],
      tools: this.toolAdapter.buildToolsDefinition()
    };

    // 2. è°ƒç”¨ AI
    const response = await this.aiService.chat(request);

    // 3. å¤„ç†å·¥å…·è°ƒç”¨
    if (response.toolCalls && response.toolCalls.length > 0) {
      const toolResults = await this.toolAdapter.handleToolCalls(
        response.toolCalls,
        context
      );

      // 4. å°†å·¥å…·ç»“æœåé¦ˆç»™ AI
      const followUp = await this.aiService.chat({
        ...request,
        messages: [
          ...request.messages,
          { role: 'assistant', content: response.content },
          { role: 'user', content: `å·¥å…·æ‰§è¡Œç»“æœ: ${JSON.stringify(toolResults)}` }
        ]
      });

      return {
        content: followUp.content,
        toolCalls: response.toolCalls,
        toolResults
      };
    }

    return {
      content: response.content
    };
  }
}
```

#### å¯¹è¯æµç¨‹ç¤ºä¾‹

```
ç”¨æˆ·: "å¸®æˆ‘æ£€æŸ¥ä¸€ä¸‹æŠ¥é”€æ•°æ®é‡Œæœ‰æ²¡æœ‰è¶…è¿‡5000å…ƒçš„å¼‚å¸¸è®°å½•"

AI: {
  "content": "å¥½çš„,æˆ‘æ¥å¸®æ‚¨æ£€æµ‹å¼‚å¸¸æŠ¥é”€è®°å½•ã€‚",
  "toolCalls": [
    {
      "name": "detect_anomalies",
      "arguments": {
        "filePath": "/mnt/æŠ¥é”€æ˜ç»†.xlsx",
        "rules": [
          { "column": "é‡‘é¢", "operator": ">", "value": 5000 }
        ]
      }
    }
  ]
}

ç³»ç»Ÿ: æ‰§è¡Œå·¥å…·...
â†’ æ‰¾åˆ° 23 ç¬”è¶…è¿‡ 5000 å…ƒçš„è®°å½•

AI: {
  "content": "æˆ‘å‘ç°äº† 23 ç¬”è¶…è¿‡ 5000 å…ƒçš„æŠ¥é”€è®°å½•ã€‚å…¶ä¸­æœ€å¤§çš„ä¸€ç¬”æ˜¯ 12,800 å…ƒ,å‘ç”Ÿåœ¨ 2025-01-15,æŠ¥é”€äººæ˜¯å¼ ä¸‰ã€‚éœ€è¦æˆ‘å¸®æ‚¨æŠŠè¿™äº›è®°å½•å¡«å…¥æŠ¥å‘Šçš„å¼‚å¸¸æ˜ç»†è¡¨å—?"
}
```

#### å®æ–½ä¼˜å…ˆçº§: **P0 (æ ¸å¿ƒç«äº‰åŠ›)**

ç†ç”±:
1. æ˜¯"å®¡è®¡åŠ©æ‰‹"çš„æ ¸å¿ƒèƒ½åŠ›
2. å¤§å¹…æå‡ç”¨æˆ·ä½“éªŒ
3. æŠ€æœ¯æ¶æ„æ¸…æ™°,é£é™©å¯æ§

---

## ğŸ¯ ç¬¬ä¸‰éƒ¨åˆ†: æŠ€æœ¯æ ˆåˆ†æ

### 3.1 Pyodide Wasm ç¯å¢ƒ

#### é¡¾é—®çš„ä½¿ç”¨åœºæ™¯

1. **è™šæ‹Ÿæ–‡ä»¶ç³»ç»Ÿ** - `/mnt` ç›®å½•æŒ‚è½½
2. **pandas/openpyxl** - æ•°æ®å¤„ç†
3. **python-docx** - æ–‡æ¡£æ“ä½œ
4. **ä»£ç æ‰§è¡Œ** - åŠ¨æ€è¿è¡Œ AI ç”Ÿæˆçš„ä»£ç 

#### å½“å‰ç³»ç»Ÿè¯„ä¼°

**å·²å®ç°** âœ…:
- PyodideService å•ä¾‹æ¨¡å¼
- åŸºç¡€åŒ…åŠ è½½ (pandas, openpyxl, numpy)
- ç›®å½•ç»“æ„ (`/data`, `/data/temp`, `/output`)
- ä»£ç æ‰§è¡Œæ¥å£

**éœ€æ”¹è¿›** âš ï¸:
1. **å†…å­˜ç®¡ç†**
   - ç¼ºå°‘æ˜¾å¼çš„å†…å­˜æ¸…ç†
   - å¤§æ–‡ä»¶å¤„ç†å¯èƒ½å´©æºƒ

2. **åŒ…ç®¡ç†**
   - æœªé¢„è£… python-docx
   - ç¼ºå°‘å¸¸ç”¨å®¡è®¡å‡½æ•°åº“

3. **æ€§èƒ½ä¼˜åŒ–**
   - æ— æ‰§è¡Œè¶…æ—¶æ§åˆ¶
   - æ— å¹¶å‘é™åˆ¶

#### å¢å¼ºæ–¹æ¡ˆ

```typescript
/**
 * å¢å¼ºçš„ Pyodide æœåŠ¡
 */
class EnhancedPyodideService extends PyodideService {
  private executionQueue: ExecutionQueue;
  private memoryMonitor: MemoryMonitor;
  private packageCache: PackageCache;

  /**
   * åˆå§‹åŒ–æ—¶é¢„è£…å®¡è®¡å·¥å…·åŒ…
   */
  protected async loadAuditPackages(): Promise<void> {
    const packages = [
      'pandas',
      'openpyxl',
      'numpy',
      'python-docx',  // æ–°å¢
      'matplotlib',   // å¯é€‰: å›¾è¡¨ç”Ÿæˆ
      'openpyxl'      // å·²æœ‰
    ];

    // åˆ†æ‰¹åŠ è½½,é¿å…é˜»å¡
    for (const pkg of packages) {
      try {
        await this.loadPackage(pkg);
        this.log('info', `Package loaded: ${pkg}`);
      } catch (error) {
        this.log('warn', `Failed to load package: ${pkg}`, { error });
      }
    }

    // å®‰è£…å®¡è®¡è¾…åŠ©å‡½æ•°
    await this.installAuditHelpers();
  }

  /**
   * å®‰è£…å®¡è®¡è¾…åŠ©å‡½æ•°åº“
   */
  private async installAuditHelpers(): Promise<void> {
    const helpersCode = `
# audit_helpers.py - å®¡è®¡å¸¸ç”¨å‡½æ•°åº“
import pandas as pd
from datetime import datetime, timedelta

def fuzzy_match_date(df1, df2, date_col1, date_col2, days=1):
    """æ—¥æœŸæ¨¡ç³ŠåŒ¹é…"""
    # ...
    pass

def detect_split_payments(df, amount_col, group_col, tolerance=0.01):
    """æ£€æµ‹æ‹†åˆ†æ”¯ä»˜"""
    # ...
    pass

def format_amount(amount, precision=2):
    """æ ¼å¼åŒ–é‡‘é¢ - åƒåˆ†ä½"""
    return f"{amount:,.{precision}f}"

def clean_column_name(col):
    """æ¸…ç†åˆ—å - å»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦"""
    return col.strip().replace(' ', '_').replace('(', '').replace(')', '')
    `;

    await this.runPython(helpersCode);
  }

  /**
   * æ‰§è¡Œ Python ä»£ç  (å¸¦èµ„æºæ§åˆ¶)
   */
  async executeWithLimits(
    code: string,
    limits: {
      timeout?: number;      // è¶…æ—¶æ—¶é—´ (ms)
      maxMemory?: number;    // æœ€å¤§å†…å­˜ (MB)
    } = {}
  ): Promise<ExecutionResult> {
    const { timeout = 30000, maxMemory = 100 } = limits;

    // 1. æ£€æŸ¥å†…å­˜
    const currentMemory = this.memoryMonitor.getCurrentUsage();
    if (currentMemory > maxMemory * 0.8) {
      await this.cleanupMemory();
    }

    // 2. æ‰§è¡Œä»£ç  (å¸¦è¶…æ—¶)
    const result = await this.executeWithTimeout(code, timeout);

    // 3. ç›‘æ§å†…å­˜å¢é•¿
    if (result.memoryUsage) {
      this.memoryMonitor.recordUsage(result.memoryUsage);
    }

    return result;
  }

  /**
   * å¸¦è¶…æ—¶çš„æ‰§è¡Œ
   */
  private async executeWithTimeout(
    code: string,
    timeout: number
  ): Promise<ExecutionResult> {
    return Promise.race([
      this.runPython(code),
      new Promise<never>((_, reject) =>
        setTimeout(() => reject(new Error('Execution timeout')), timeout)
      )
    ]);
  }

  /**
   * å†…å­˜æ¸…ç†
   */
  private async cleanupMemory(): Promise<void> {
    this.log('info', 'Cleaning up memory...');

    await this.runPython(`
import gc
gc.collect()

# æ¸…ç†å¤§å¯¹è±¡
import sys
for obj in gc.get_objects():
    if isinstance(obj, pd.DataFrame) and sys.getsizeof(obj) > 10_000_000:
        del obj
    `);
  }
}
```

#### å®æ–½ä¼˜å…ˆçº§: **P0 (åŸºç¡€ä¼˜åŒ–)**

---

### 3.2 Python åº“é›†æˆ

#### python-docx é›†æˆ

**éœ€æ±‚**: å®ç°é¡¾é—®çš„ç²¾ç¡®æ–‡æ¡£æ§åˆ¶æ–¹æ¡ˆ

**æ–¹æ¡ˆ**:
```typescript
/**
 * Python-docx æœåŠ¡åŒ…è£…
 */
class PythonDocxService {
  /**
   * å¡«å……è¡¨æ ¼ (é¡¾é—®æ–¹æ¡ˆ)
   */
  async fillTableWithDF(
    templatePath: string,
    tableIndex: number,
    data: any[][],
    startRow: number = 1
  ): Promise<void> {
    const script = `
from docx import Document

doc = Document('${templatePath}')
table = doc.tables[${tableIndex}]

# æ ·æœ¬è¡Œ
sample_cells = table.rows[${startRow}].cells

# æ•°æ®
data = ${JSON.stringify(data)}

# å¡«å……
for i, row_data in enumerate(data):
    current_row = table.rows[${startRow} + i]

    for j, value in enumerate(row_data):
        if j < len(current_row.cells):
            cell = current_row.cells[j]
            paragraph = cell.paragraphs[0]
            paragraph.text = str(value)

            # æ ·å¼å…‹éš†
            if i > 0 and j < len(sample_cells):
                sample_para = sample_cells[j].paragraphs[0]
                if sample_para.runs:
                    sample_run = sample_para.runs[0]
                    for run in paragraph.runs:
                        run.font.name = sample_run.font.name
                        run.font.size = sample_run.font.size

doc.save('/data/temp/filled.docx')
    `;

    await this.pyodideService.runPython(script);
  }
}
```

#### å®æ–½ä¼˜å…ˆçº§: **P1 (å¯é€‰å¢å¼º)**

---

### 3.3 Gemini API é›†æˆ

#### é¡¾é—®å»ºè®®çš„åŠŸèƒ½

1. **é•¿æ–‡æœ¬èƒ½åŠ›** - å¤„ç†å¤æ‚æ–‡æ¡£
2. **Function Calling** - å·¥å…·è°ƒç”¨
3. **å¤šè½®å¯¹è¯** - è¿­ä»£ä¼˜åŒ–

#### å½“å‰ç³»ç»Ÿè¯„ä¼°

**ç°çŠ¶**:
- ä½¿ç”¨æ™ºè°± AI (glm-4.6)
- åŸºç¡€ API è°ƒç”¨å·²å®ç°
- æ—  Function Calling

**å…¼å®¹æ€§è¯„ä¼°**:
- âœ… æ™ºè°± AI ä¹Ÿæ”¯æŒ Function Calling
- âœ… é•¿æ–‡æœ¬èƒ½åŠ›ç›¸å½“ (128K context)
- âš ï¸ éœ€è¦é€‚é… API å·®å¼‚

#### å®æ–½æ–¹æ¡ˆ

```typescript
/**
 * æ™ºè°± AI Function Calling é€‚é…å™¨
 */
class ZhipuFunctionCallingAdapter {
  /**
   * è°ƒç”¨å¸¦å·¥å…·çš„èŠå¤©æ¥å£
   */
  async chatWithTools(
    messages: ChatMessage[],
    tools: ToolDefinition[]
  ): Promise<ChatResponse> {
    const response = await this.client.chat.completions.create({
      model: 'glm-4.6',
      messages,
      tools: tools.map(tool => ({
        type: 'function',
        function: {
          name: tool.name,
          description: tool.description,
          parameters: tool.parameters
        }
      }))
    });

    return this.parseResponse(response);
  }

  /**
   * è§£æå“åº”
   */
  private parseResponse(response: any): ChatResponse {
    const message = response.choices[0].message;

    // æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
    if (message.tool_calls) {
      return {
        content: message.content,
        toolCalls: message.tool_calls.map((call: any) => ({
          id: call.id,
          name: call.function.name,
          arguments: JSON.parse(call.function.arguments)
        }))
      };
    }

    return {
      content: message.content
    };
  }
}
```

#### å®æ–½ä¼˜å…ˆçº§: **P0 (æ ¸å¿ƒåŠŸèƒ½)**

---

## ğŸ“Š ç¬¬å››éƒ¨åˆ†: æ¶æ„ä¼˜åŒ–å»ºè®®

### 4.1 æ¨¡å—è§£è€¦æ–¹æ¡ˆ

#### å½“å‰é—®é¢˜

**ç´§è€¦åˆç¤ºä¾‹**:
```typescript
// AgenticOrchestrator ç›´æ¥ä¾èµ–å…·ä½“å®ç°
import { generateDataProcessingCode } from '../zhipuService';
import { executeTransformation } from '../excelService';
```

#### è§£è€¦æ–¹æ¡ˆ: ä¾èµ–æ³¨å…¥

```typescript
/**
 * ä½¿ç”¨æ¥å£æŠ½è±¡
 */
interface ICodeGenerator {
  generateCode(request: GenerationRequest): Promise<string>;
}

interface ICodeExecutor {
  execute(code: string, context: ExecutionContext): Promise<ExecutionResult>;
}

/**
 * ç¼–æ’å™¨ä¾èµ–æŠ½è±¡,ä¸ä¾èµ–å…·ä½“å®ç°
 */
class AgenticOrchestrator {
  constructor(
    private codeGenerator: ICodeGenerator,
    private codeExecutor: ICodeExecutor,
    private aiService: IAIService
  ) {}
}

/**
 * å·¥å‚æ¨¡å¼åˆ›å»ºå®ä¾‹
 */
class OrchestratorFactory {
  static create(config: OrchestratorConfig): AgenticOrchestrator {
    return new AgenticOrchestrator(
      new ZhipuCodeGenerator(config.apiKey),
      new WasmCodeExecutor(config.pyodide),
      new ZhipuAIService(config.apiKey)
    );
  }
}
```

### 4.2 æ¥å£è®¾è®¡å»ºè®®

#### æ ¸å¿ƒæ¥å£å®šä¹‰

```typescript
/**
 * å…ƒæ•°æ®æå–æœåŠ¡æ¥å£
 */
interface IMetadataExtractionService {
  scoutExcel(file: File): Promise<ExcelMetadata>;
  scoutWord(file: File): Promise<WordMetadata>;
  extractRules(file: File): Promise<ControlRule[]>;
}

/**
 * é¢„å®¡å¼•æ“æ¥å£
 */
interface IPreFilterEngine {
  execute(
    data: DataSourceInfo[],
    rules: ControlRule[]
  ): Promise<PreFilterResult>;
}

/**
 * æ–‡æ¡£å¡«å……æ¥å£
 */
interface IDocumentFiller {
  fill(
    template: File,
    data: MappingData,
    options: FillOptions
  ): Promise<FilledDocument>;
}

/**
 * è‡ªæ„ˆå¼•æ“æ¥å£
 */
interface ISelfHealingEngine {
  heal(
    error: Error,
    failedCode: string,
    context: ExecutionContext
  ): Promise<HealingResult>;
}
```

### 4.3 æ•°æ®æµä¼˜åŒ–

#### ä¸­é—´æ€ç¼“å­˜ç­–ç•¥

```typescript
/**
 * æ£€æŸ¥ç‚¹ç®¡ç†å™¨
 */
class CheckpointManager {
  private checkpoints: Map<string, Checkpoint>;

  /**
   * ä¿å­˜æ£€æŸ¥ç‚¹
   */
  async save(
    stageId: string,
    data: any,
    metadata?: any
  ): Promise<void> {
    const checkpoint: Checkpoint = {
      id: this.generateId(),
      stageId,
      timestamp: Date.now(),
      dataSize: JSON.stringify(data).length,
      metadata
    };

    // ä¿å­˜åˆ°è™šæ‹Ÿæ–‡ä»¶ç³»ç»Ÿ
    const path = `/data/temp/checkpoint_${stageId}.json`;
    await this.fileSystem.writeFile(path, JSON.stringify(data));

    this.checkpoints.set(stageId, checkpoint);
  }

  /**
   * åŠ è½½æ£€æŸ¥ç‚¹
   */
  async load(stageId: string): Promise<any> {
    const checkpoint = this.checkpoints.get(stageId);
    if (!checkpoint) {
      throw new Error(`No checkpoint found: ${stageId}`);
    }

    const path = `/data/temp/checkpoint_${stageId}.json`;
    const data = await this.fileSystem.readFile(path);
    return JSON.parse(data);
  }
}
```

### 4.4 æ€§èƒ½è€ƒè™‘

#### å†…å­˜ä¼˜åŒ–

```typescript
/**
 * æµå¼å¤„ç†å¤§æ•°æ®é›†
 */
class StreamingDataProcessor {
  /**
   * åˆ†å—å¤„ç† Excel
   */
  async processInChunks(
    file: File,
    processor: (chunk: any[]) => Promise<any>,
    chunkSize: number = 1000
  ): Promise<any[]> {
    const results: any[] = [];

    // ä½¿ç”¨ chunker åˆ†å—è¯»å–
    const chunks = await this.chunkExcel(file, chunkSize);

    for (const chunk of chunks) {
      const result = await processor(chunk);
      results.push(result);

      // é‡Šæ”¾å†…å­˜
      await this.yieldToGC();
    }

    return results;
  }

  private async yieldToGC(): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, 0));
  }
}
```

#### Token ä¼˜åŒ–

```typescript
/**
 * æ™ºèƒ½ä¸Šä¸‹æ–‡å‹ç¼©
 */
class ContextCompressor {
  /**
   * å‹ç¼©å…ƒæ•°æ®
   */
  compressMetadata(metadata: FileMetadata): CompressedMetadata {
    return {
      filename: metadata.filename,
      // åªä¿ç•™å…³é”®ä¿¡æ¯
      columns: Object.keys(metadata.columns),
      sampleCount: 3,
      hasIssues: metadata.quality.hasIssues
      // çœç•¥è¯¦ç»†æ ·ä¾‹
    };
  }

  /**
   * åˆ†å±‚æ³¨å…¥
   */
  buildLayeredPrompt(
    basePrompt: string,
    metadata: FileMetadata[],
    tokenBudget: number
  ): string {
    // 1. åŸºç¡€å±‚: æ–‡ä»¶æ¸…å•
    const layer1 = this.buildFileList(metadata);

    // 2. è¯¦ç»†å±‚: æ ¹æ®é¢„ç®—é€‰æ‹©æ€§æ·»åŠ 
    const remainingTokens = tokenBudget - this.estimateTokens(layer1);
    const layer2 = this.buildDetailedMetadata(metadata, remainingTokens);

    return `${layer1}\n\n${layer2}\n\n${basePrompt}`;
  }
}
```

---

## ğŸš€ ç¬¬äº”éƒ¨åˆ†: å®æ–½è·¯çº¿å›¾

### Phase 1: åŸºç¡€å¢å¼º (2-3å‘¨)

**ç›®æ ‡**: å¤¯å®åŸºç¡€èƒ½åŠ›

#### Sprint 1: è™šæ‹Ÿå·¥ä½œå°ä¼˜åŒ–
- [ ] å®ç° `EnhancedFileSystemService`
- [ ] æ·»åŠ æ–‡ä»¶è§’è‰²æ ‡è®°
- [ ] å®ç°å…³ç³»å›¾è°±
- [ ] ä¼˜åŒ–ç›®å½•ç»“æ„

#### Sprint 2: ä¾¦å¯Ÿå…µè„šæœ¬å¢å¼º
- [ ] å®ç° `ExcelScoutService` æ·±åº¦å…ƒæ•°æ®æå–
- [ ] å®ç° `WordScoutService` ç»“æ„åˆ†æ
- [ ] æ·»åŠ æ¨¡å¼æ£€æµ‹
- [ ] æ·»åŠ è´¨é‡è¯„ä¼°

#### Sprint 3: Prompt å¢å¼º
- [ ] å®ç° `PromptEnhancementService`
- [ ] åŠ¨æ€ä¸Šä¸‹æ–‡æ³¨å…¥
- [ ] æ™ºèƒ½çº¦æŸç”Ÿæˆ
- [ ] Few-Shot ç¤ºä¾‹ä¼˜åŒ–

### Phase 2: æ ¸å¿ƒåŠŸèƒ½ (3-4å‘¨)

**ç›®æ ‡**: å®ç°å…³é”®èƒ½åŠ›

#### Sprint 4: æ•°æ®æµç¼–æ’
- [ ] å®ç° `DataFlowOrchestrator`
- [ ] ç®¡é“å¼å¤„ç†æµç¨‹
- [ ] æ£€æŸ¥ç‚¹æœºåˆ¶
- [ ] æ–­ç‚¹ç»­ä¼ 

#### Sprint 5: æ€»æ§å¼•æ“
- [ ] å®ç° `EnhancedAgenticOrchestrator`
- [ ] å››é˜¶æ®µæ‰§è¡Œæ¨¡å‹
- [ ] èåˆ OTAE å¾ªç¯
- [ ] å¯é€‰å†…æ§æ¨¡å¼

#### Sprint 6: è‡ªæ„ˆé€»è¾‘
- [ ] å®ç° `SelfHealingEngine`
- [ ] é”™è¯¯åˆ†ç±»ç³»ç»Ÿ
- [ ] æ™ºèƒ½ä¿®å¤ç­–ç•¥
- [ ] é‡è¯•ä¼˜åŒ–

### Phase 3: é«˜çº§åŠŸèƒ½ (3-4å‘¨)

**ç›®æ ‡**: æ‰“é€ å·®å¼‚åŒ–ç«äº‰åŠ›

#### Sprint 7: å†…æ§é¢„å®¡
- [ ] å®ç° `InternalControlPreFilterEngine`
- [ ] è§„åˆ™æå–å’Œè§£æ
- [ ] å¼‚å¸¸ç­›é€‰
- [ ] é£é™©è¯„åˆ†

#### Sprint 8: Function Calling
- [ ] å®ç° `FunctionCallingAdapter`
- [ ] å·¥å…·æ³¨å†Œè¡¨
- [ ] æ™ºèƒ½å¯¹è¯æœåŠ¡
- [ ] å·¥å…·æ‰§è¡Œåé¦ˆ

#### Sprint 9: Python-docx é›†æˆ
- [ ] å®ç° `PythonDocxService`
- [ ] ç²¾ç¡®æ ¼å¼æ§åˆ¶
- [ ] åˆå¹¶å•å…ƒæ ¼æ”¯æŒ
- [ ] æ€§èƒ½ä¼˜åŒ–

### Phase 4: å®Œå–„ä¼˜åŒ– (2-3å‘¨)

**ç›®æ ‡**: ç”Ÿäº§å°±ç»ª

#### Sprint 10: æ€§èƒ½ä¼˜åŒ–
- [ ] å†…å­˜ç®¡ç†ä¼˜åŒ–
- [ ] æµå¼å¤„ç†
- [ ] Token å‹ç¼©
- [ ] ç¼“å­˜ç­–ç•¥

#### Sprint 11: ç”¨æˆ·ä½“éªŒ
- [ ] è¿›åº¦åé¦ˆä¼˜åŒ–
- [ ] é”™è¯¯æç¤ºä¼˜åŒ–
- [ ] äº¤äº’æµç¨‹ä¼˜åŒ–
- [ ] æ–‡æ¡£å®Œå–„

#### Sprint 12: æµ‹è¯•å’Œéƒ¨ç½²
- [ ] å•å…ƒæµ‹è¯•å®Œå–„
- [ ] é›†æˆæµ‹è¯•
- [ ] æ€§èƒ½æµ‹è¯•
- [ ] ç”Ÿäº§éƒ¨ç½²

---

## âš ï¸ ç¬¬å…­éƒ¨åˆ†: æŠ€æœ¯é£é™©è¯„ä¼°

### é«˜é£é™©é¡¹

#### 1. Pyodide å†…å­˜é™åˆ¶ (é£é™©ç­‰çº§: ğŸ”´ é«˜)

**é—®é¢˜**:
- æµè§ˆå™¨å†…å­˜æœ‰é™ (é€šå¸¸ < 2GB)
- å¤§æ–‡ä»¶å¤„ç†å¯èƒ½å¯¼è‡´å´©æºƒ

**ç¼“è§£ç­–ç•¥**:
- âœ… æµå¼å¤„ç† (åˆ†å—è¯»å–)
- âœ… æ˜¾å¼å†…å­˜æ¸…ç†
- âœ… æ–‡ä»¶å¤§å°é™åˆ¶ (å»ºè®® < 50MB)
- âœ… æä¾›é™çº§æ–¹æ¡ˆ (åç«¯å¤„ç†)

#### 2. AI è¾“å‡ºç¨³å®šæ€§ (é£é™©ç­‰çº§: ğŸŸ¡ ä¸­)

**é—®é¢˜**:
- AI ç”Ÿæˆçš„ä»£ç å¯èƒ½ä¸æ­£ç¡®
- éœ€è¦å¤šè½®è¿­ä»£

**ç¼“è§£ç­–ç•¥**:
- âœ… è‡ªæ„ˆé€»è¾‘
- âœ… Few-Shot ç¤ºä¾‹
- âœ… è¾“å‡ºéªŒè¯
- âœ… äººå·¥ç¡®è®¤æœºåˆ¶

#### 3. è·¨æµè§ˆå™¨å…¼å®¹æ€§ (é£é™©ç­‰çº§: ğŸŸ¡ ä¸­)

**é—®é¢˜**:
- Pyodide åœ¨ä¸åŒæµè§ˆå™¨è¡¨ç°ä¸åŒ
- æŸäº›ç‰¹æ€§å¯èƒ½ä¸æ”¯æŒ

**ç¼“è§£ç­–ç•¥**:
- âœ… ç‰¹æ€§æ£€æµ‹
- âœ… ä¼˜é›…é™çº§
- âœ… å¤šæµè§ˆå™¨æµ‹è¯•
- âœ… æä¾›å…¼å®¹æ€§çŸ©é˜µ

### ä½é£é™©é¡¹

- âœ… æ¨¡å—è§£è€¦ - æ¸è¿›å¼é‡æ„,é£é™©å¯æ§
- âœ… æ¥å£è®¾è®¡ - ä¸å½±å“ç°æœ‰åŠŸèƒ½
- âœ… ç¼“å­˜ä¼˜åŒ– - çº¯å¢å¼º,æ— ç ´åæ€§

---

## ğŸ“ˆ ç¬¬ä¸ƒéƒ¨åˆ†: æˆåŠŸæŒ‡æ ‡

### æŠ€æœ¯æŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | æµ‹é‡æ–¹å¼ |
|------|------|------|---------|
| ä»£ç ç”ŸæˆæˆåŠŸç‡ | ~60% | >85% | ç»Ÿè®¡é¦–æ¬¡æ‰§è¡ŒæˆåŠŸç‡ |
| è‡ªæ„ˆä¿®å¤ç‡ | 0% | >70% | ç»Ÿè®¡è‡ªåŠ¨ä¿®å¤æˆåŠŸæ¬¡æ•° |
| å¹³å‡å¤„ç†æ—¶é—´ | æœªçŸ¥ | <30s | ç«¯åˆ°ç«¯è®¡æ—¶ |
| å†…å­˜ä½¿ç”¨å³°å€¼ | æœªçŸ¥ | <500MB | æ€§èƒ½ç›‘æ§ |
| é”™è¯¯æ¢å¤æ—¶é—´ | N/A | <10s | é”™è¯¯å¤„ç†è®¡æ—¶ |

### ä¸šåŠ¡æŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | æµ‹é‡æ–¹å¼ |
|------|------|------|---------|
| ç”¨æˆ·æ»¡æ„åº¦ | æœªçŸ¥ | >4.0/5.0 | ç”¨æˆ·è°ƒç ” |
| åŠŸèƒ½ä½¿ç”¨ç‡ | æœªçŸ¥ | >60% | è¡Œä¸ºåˆ†æ |
| æ”¯æŒå·¥å•å‡å°‘ | æœªçŸ¥ | -50% | å·¥å•ç»Ÿè®¡ |
| å®¡è®¡æ•ˆç‡æå‡ | æœªçŸ¥ | +200% | ç”¨æˆ·åé¦ˆ |

---

## ğŸ¯ ç¬¬å…«éƒ¨åˆ†: æ€»ç»“ä¸å»ºè®®

### æ ¸å¿ƒä»·å€¼

è¿™ä»½é¡¾é—®äº¤æµè®°å½•æä¾›äº†**æå…·å‰ç»æ€§çš„æ¶æ„è®¾è®¡è“å›¾**,å…¶æ ¸å¿ƒä»·å€¼åœ¨äº:

1. **ç³»ç»Ÿæ€§æ€ç»´** - ä»å•ç‚¹å·¥å…·å‡çº§ä¸ºç”Ÿæ€ç³»ç»Ÿ
2. **å®¡è®¡ä¸“ä¸šæ€§** - æ·±åˆ»ç†è§£å®¡è®¡å·¥ä½œæœ¬è´¨
3. **æŠ€æœ¯å¯è¡Œæ€§** - æ–¹æ¡ˆåŠ¡å®,å¯ç›´æ¥è½åœ°
4. **æ¶æ„ä¼˜é›…æ€§** - ä¸ç°æœ‰æ¶æ„é«˜åº¦å¥‘åˆ

### å…³é”®å»ºè®®

#### å¿…é¡»å®æ–½ (P0)
1. âœ… è™šæ‹Ÿå·¥ä½œå°ä¼˜åŒ–
2. âœ… ä¾¦å¯Ÿå…µè„šæœ¬å¢å¼º
3. âœ… æ•°æ®æµç¼–æ’å™¨
4. âœ… æ€»æ§å¼•æ“å››é˜¶æ®µæ¨¡å‹
5. âœ… è‡ªæ„ˆé€»è¾‘å®Œå–„
6. âœ… Function Calling é€‚é…å™¨

#### åº”è¯¥å®æ–½ (P1)
1. â­ å†…æ§é¢„å®¡å¼•æ“
2. â­ Python-docx é›†æˆ
3. â­ æ€§èƒ½ä¼˜åŒ–
4. â­ ç”¨æˆ·ä½“éªŒæå‡

#### å¯ä»¥å»¶å (P2)
1. ğŸ’¡ é«˜çº§å›¾è¡¨åŠŸèƒ½
2. ğŸ’¡ å¤šè¯­è¨€æ”¯æŒ
3. ğŸ’¡ æ’ä»¶ç³»ç»Ÿ

### æœ€ç»ˆè¯„ä»·

**æ¶æ„å…¼å®¹æ€§**: â­â­â­â­â­ (95%)
**æŠ€æœ¯å¯è¡Œæ€§**: â­â­â­â­â˜† (80%)
**ä¸šåŠ¡ä»·å€¼**: â­â­â­â­â­ (100%)
**å®æ–½é£é™©**: â­â­â˜†â˜†â˜† (30%)

**æ€»ä½“ç»“è®º**: å¼ºçƒˆå»ºè®®æŒ‰æ­¤æ–¹æ¡ˆè¿›è¡Œç¬¬äºŒé˜¶æ®µä¼˜åŒ–,é¢„è®¡å¯å°†ç³»ç»Ÿä»"å·¥å…·"æå‡ä¸º"æ™ºèƒ½å®¡è®¡åŠ©æ‰‹",åœ¨å¸‚åœºä¸Šå½¢æˆæ˜¾è‘—ç«äº‰ä¼˜åŠ¿ã€‚

---

**æ–‡æ¡£ç»“æŸ**

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:
1. ä¸å›¢é˜Ÿè®¨è®ºæœ¬æŠ¥å‘Š
2. ç¡®å®š Phase 1 è¯¦ç»†è®¡åˆ’
3. åˆ†é… Sprint ä»»åŠ¡
4. å¯åŠ¨å®æ–½

**é¢„æœŸæˆæœ**:
- 3ä¸ªæœˆå†…å®Œæˆæ ¸å¿ƒåŠŸèƒ½
- ç³»ç»Ÿèƒ½åŠ›æå‡ 200%+
- ç”¨æˆ·æ»¡æ„åº¦æ˜¾è‘—æå‡
- å¸‚åœºç«äº‰åŠ›å¤§å¹…å¢å¼º
